---
title: "AIRTOOL PROTOTYPE--not intended for public consumption"
format: 
  dashboard:
    theme: spacelab
    orientation: columns
    # embed-resources: true
    header-includes:
      - '<meta name="color-scheme" content="light">'

resources:
  - readme_md_files
server: shiny
---

```{=html}
<style>
  .plot-container, .shiny-text-output, .panel, .card {
    border: none;
    box-shadow: none;
  }
  .custom-text {
    font-size: 20px;
    color: green;
    font-weight: bold;
  }
</style>

<style> .main-container { max-width: unset; } </style>
<div id="style-test" style="background-color: white; color: black; position: absolute; left: -9999px;"></div>

<!-- Hidden element for detecting overridden styles -->
<div id="style-test" style="background-color: white; color: black; display: none;"></div>

<script>
document.addEventListener("DOMContentLoaded", () => {
  const testElem = document.getElementById("style-test");
  if (!testElem) return;

  const computedBg = window.getComputedStyle(testElem).backgroundColor;
  const computedColor = window.getComputedStyle(testElem).color;

  if (computedBg !== "rgb(255, 255, 255)" || computedColor !== "rgb(0, 0, 0)") {
    alert("It appears a dark mode extension might be overriding the styles. Please disable it for the best experience.");
  }
});
</script>
```

```{r initial-setup}
#| context: setup
#| echo: false
#| include: false

# suppressMessages(library(tidyverse))
suppressMessages(library(AIPW))
suppressMessages(library(DiagrammeR))
suppressMessages(library(dplyr))
suppressMessages(library(e1071))
suppressMessages(library(ggplot2))
suppressMessages(library(hal9001))
suppressMessages(library(here))
suppressMessages(library(nnet))
suppressMessages(library(randomForest))
suppressMessages(library(readr))
suppressMessages(library(rJava))
suppressMessages(library(rpart))
suppressMessages(library(scales))
suppressMessages(library(shiny))
suppressMessages(library(shinyWidgets))
suppressMessages(library(sl3))
suppressMessages(library(tidyr))
suppressMessages(library(tmle3))
suppressMessages(library(xgboost))
suppressMessages(library(jsonlite))
suppressMessages(library(foreach))
suppressMessages(library(doParallel))
suppressMessages(library(earth))
suppressMessages(library(hash))
suppressMessages(library(sets))
suppressMessages(library(base64enc))

#options(warn = 2)

AIRHome <- here()
setwd(AIRHome)
set.seed(123)

# --- Buffers --- #
results_buffer <- reactiveVal(data.frame())
results_out_buffer <- reactiveVal(data.frame())
datafile_buffer <- reactiveVal(NULL)
superlearner_output_buffer <- reactiveVal(list())
graphtxt_buffer <- reactiveVal(NULL)
dotfile_buffer <- reactiveVal(NULL)
rv <- reactiveValues()

# ── NEW: keep TMLE rows, ML rows, and the 1-row summary apart ────────────────
tmle_buffer     <- reactiveVal(data.frame())   # 1 row per adjustment set
ml_buffer       <- reactiveVal(data.frame())   # 1 row per ML algorithm
combined_buffer <- reactiveVal(data.frame())   # exactly 1 row (ribbon plot)

# create a shared reactiveValues object to pass variables around
rv$dot <- NULL  # define once in app init

options(shiny.trace = TRUE)

# --- BEGIN INLINE: scripts/AIR_functions.R ---
getLocalTags <- function() {
  if (!isLocal()) {
    return(NULL)
  }
  
  htmltools::tagList(
    htmltools::tags$script(paste0(
      "$(function() {",
      "  $(document).on('shiny:disconnected', function(event) {",
      "    $('#ss-connect-dialog').show();",
      "    $('#ss-overlay').show();",
      "  })",
      "});"
    )),
    htmltools::tags$div(
      id="ss-connect-dialog", style="display: none;",
      htmltools::tags$a(id="ss-reload-link", href="#", onclick="window.location.reload(true);")
    ),
    htmltools::tags$div(id="ss-overlay", style="display: none;")
  )
}

isLocal <- function() {
  Sys.getenv("SHINY_PORT", "") == ""
}

disconnectMessage <- function(
    text = "An error occurred. Please refresh the page and try again.",
    refresh = "Refresh",
    width = 450,
    top = 50,
    size = 22,
    background = "white",
    colour = "#444444",
    overlayColour = "black",
    overlayOpacity = 0.6,
    refreshColour = "#337ab7",
    css = ""
) {
  
  checkmate::assert_string(text, min.chars = 1)
  checkmate::assert_string(refresh)
  checkmate::assert_numeric(size, lower = 0)
  checkmate::assert_string(background)
  checkmate::assert_string(colour)
  checkmate::assert_string(overlayColour)
  checkmate::assert_number(overlayOpacity, lower = 0, upper = 1)
  checkmate::assert_string(refreshColour)
  checkmate::assert_string(css)
  
  if (width == "full") {
    width <- "100%"
  } else if (is.numeric(width) && width >= 0) {
    width <- paste0(width, "px")
  } else {
    stop("disconnectMessage: 'width' must be either an integer, or the string \"full\".", call. = FALSE)
  }
  
  if (top == "center") {
    top <- "50%"
    ytransform <- "-50%"
  } else if (is.numeric(top) && top >= 0) {
    top <- paste0(top, "px")
    ytransform <- "0"
  } else {
    stop("disconnectMessage: 'top' must be either an integer, or the string \"center\".", call. = FALSE)
  }
  
  htmltools::tagList(
    getLocalTags(),
    htmltools::tags$head(
      htmltools::tags$style(
        glue::glue(
          .open = "{{", .close = "}}",
          
          "#shiny-disconnected-overlay { display: none !important; }",
          
          "#ss-overlay {
             background-color: {{overlayColour}} !important;
             opacity: {{overlayOpacity}} !important;
             position: fixed !important;
             top: 0 !important;
             left: 0 !important;
             bottom: 0 !important;
             right: 0 !important;
             z-index: 99998 !important;
             overflow: hidden !important;
             cursor: not-allowed !important;
          }",
          
          "#ss-connect-dialog {
             background: {{background}} !important;
             color: {{colour}} !important;
             width: {{width}} !important;
             transform: translateX(-50%) translateY({{ytransform}}) !important;
             font-size: {{size}}px !important;
             top: {{top}} !important;
             position: fixed !important;
             bottom: auto !important;
             left: 50% !important;
             padding: 0.8em 1.5em !important;
             text-align: center !important;
             height: auto !important;
             opacity: 1 !important;
             z-index: 99999 !important;
             border-radius: 3px !important;
             box-shadow: rgba(0, 0, 0, 0.3) 3px 3px 10px !important;
          }",
          
          "#ss-connect-dialog::before { content: '{{text}}' }",
          
          "#ss-connect-dialog label { display: none !important; }",
          
          "#ss-connect-dialog a {
             display: {{ if (refresh == '') 'none' else 'block' }} !important;
             color: {{refreshColour}} !important;
             font-size: 0 !important;
             margin-top: {{size}}px !important;
             font-weight: normal !important;
          }",
          
          "#ss-connect-dialog a::before {
            content: '{{refresh}}';
            font-size: {{size}}px;
          }",
          
          "#ss-connect-dialog { {{ htmltools::HTML(css) }} }"
        )
      )
    )
  )
}

#' Show a nice message when a shiny app disconnects or errors
#'
#' This function is a version of disconnectMessage() with a pre-set combination
#' of parameters that results in a large centered message.
#' @export
disconnectMessage2 <- function() {
  disconnectMessage(
    text = "Your session has timed out.",
    refresh = "",
    size = 70,
    colour = "white",
    background = "rgba(64, 64, 64, 0.9)",
    width = "full",
    top = "center",
    overlayColour = "#999",
    overlayOpacity = 0.7,
    css = "padding: 15px !important; box-shadow: none !important;"
  )
}

fix_knowledge <- function(df){
  # Store original column names
  original_colnames <- colnames(df)
  
  # Detect numeric vs non-numeric columns
  numeric_cols <- sapply(df, function(col) all(!is.na(suppressWarnings(as.numeric(as.character(col))))))
  # check if column header is missing and data conform to expectations. If so, process and return
  if (any(!is.na(suppressWarnings(as.numeric(original_colnames))))) {
    
    # Confirm exactly one numeric and one character-type column exist
    if (sum(numeric_cols) == 1 && sum(!numeric_cols) == 1) {
      new_colnames <- c("level", "variable")
      new_colnames_ordered <- rep(NA, length(df))
      new_colnames_ordered[numeric_cols] <- "level"
      new_colnames_ordered[!numeric_cols] <- "variable"
      
      # Move original column names to first row
      df <- rbind(setNames(as.list(original_colnames), names(df)), df)
      
      # Now assign the new column names
      colnames(df) <- new_colnames_ordered
    } else {
      return("Unable to read knowledge file data. Please make sure file contains a header with the following column names: level, variable. 'variable' should contain the name of each variable used, and 'level' should be a numeric value to represent an estimated causal hierarchy (see readme file for a detailed description).")
    }  
  } else if (sum(numeric_cols) == 1 && sum(!numeric_cols) == 1) {
      colnames(df)[numeric_cols] <- "level"
      colnames(df)[!numeric_cols] <- "variable"
    } else {
      return("Unable to read knowledge file data. Please make sure file contains a header with the following column names: level, variable. 'variable' should contain the name of each variable used, and 'level' should be a numeric value to represent an estimated causal hierarchy (see readme file for a detailed description).")
    }
  
  return(df)
}

# change color of nodes in graph
# change_node_color <- function(dot_code, node, color) {
#   for (i in 1:length(node)) {
#     # Create the node definition with the color
#     node_definition <- paste0("\"", node[i], "\" [style=filled, fillcolor=", color, "];")
#     
#     # Append the new node definition to the beginning of the DOT code
#     dot_code <- sub("digraph g \\{", paste0("digraph g {\r\n  ", node_definition), dot_code)
#   }
#   
#   return(dot_code)
# }
# 
# change_node_color <- function(dot_code, node, color) {
#   # Create all node definitions as a single string with a single newline separator
#   node_definitions <- paste0("\"", node, "\" [style=filled, fillcolor=", color, "];", collapse = "\n  ")
#   
#   # Replace once, inserting all node definitions
#   dot_code <- sub("digraph g \\{", paste0("digraph g {\n  ", node_definitions), dot_code)
#   
#   return(dot_code)
# }

change_node_color <- function(dot_code, node, color) {
  # Remove any accidental extra quotes from the color string
  color <- trimws(gsub("['\"]", "", color))
  node_definition <- paste0("\"", node, "\" [style=filled, fillcolor=\"", color, "\"];")
  dot_code <- sub("digraph g \\{", paste0("digraph g {\n  ", node_definition), dot_code)
  dot_code <- gsub("\'", "\"", dot_code)
  return(dot_code)
}

AIR_getGraph <- function(data, knowledge, graphtxt_buffer = NULL) {
  headers_string <- "PD\tfrac_ind\tfrac_dep\tunif\t \tBIC\t \t#edges\tn_tests_ind\tn_tests_dep"
  cat(headers_string, "\n")

  MC_passing_cpdag_already_found <- FALSE
  best_cpdag_seen_so_far <- NULL
  best_cpdag_seen_so_far_num_edges <- Inf
  best_cpdag_seen_so_far_params <- "<unset>"
  cpdag_graph_when_PD_is_1 <- NULL
  last_ts <- NULL

  suppress_output <- function(expr) {
    sink("/dev/null")
    on.exit(sink(), add = TRUE)
    force(expr)
  }

  for (i in seq(0, 15)) {
    pd <- 0.5 + (i * 0.1)

    tryCatch({
      ts <- TetradSearch$new(data)
      last_ts <- ts

      # Validate and apply knowledge tiers
      if (!is.null(knowledge) && nrow(knowledge) > 0) {
        if (!all(c("level", "variable") %in% colnames(knowledge))) {
          warning("Knowledge file is missing required columns 'level' and 'variable'; skipping tier assignment.")
        } else {
          # Add knowledge to specific tiers
          for (j in seq_len(nrow(knowledge))) {
            level <- knowledge$level[j]
            variable <- knowledge$variable[j]
            
            if (is.numeric(level) && is.character(variable)) {
              tryCatch({
                ts$add_to_tier(level, variable)
              }, error = function(e) {
                warning(sprintf("Failed to add variable '%s' to tier %s: %s", 
                                as.character(variable), as.character(level), conditionMessage(e)))
              })
            } else {
              warning(sprintf("Invalid knowledge row at index %d: level=%s, variable=%s",
                              j, deparse(level), deparse(variable)))
            }
          }
        }
      }
      
      ts$use_sem_bic(penalty_discount = pd)

      suppress_output(ts$run_boss())
      g2 <- ts$get_java()

      bic <- tryCatch(g2$getAttribute("BIC"), error = function(e) NA_real_)
      num_edges <- tryCatch(g2$getNumEdges(), error = function(e) NA_integer_)

      suppress_output(ts$use_fisher_z(use_for_mc = TRUE))
      ret <- suppress_output(ts$markov_check(g2))

      cpdag_graph_when_PD_is_1 <- g2

      result_line <- tryCatch({
        sprintf(
          "%.1f\t%.4f  \t%.4f   \t%.4f  \t%.2f  \t%.0f  \t%.0f  \t\t%.0f",
          pd, ret$frac_dep_ind, ret$frac_dep_dep, ret$ad_ind, bic, num_edges,
          ret$num_tests_ind, ret$num_tests_dep
        )
      }, error = function(e) {
        "<incomplete>"
      })

      if (!is.null(ret) && is.list(ret) && !is.null(ret$ad_ind) && ret$ad_ind > 0.1) {
        cat(result_line, "\n")

        if (!MC_passing_cpdag_already_found || (num_edges < best_cpdag_seen_so_far_num_edges)) {
          best_cpdag_seen_so_far <- g2
          best_cpdag_seen_so_far_num_edges <- num_edges
          best_cpdag_seen_so_far_params <- result_line
          MC_passing_cpdag_already_found <- TRUE
        }
      }

      # If PD==1.0 and still no valid CPDAG found, keep something usable
      if (pd == 1.0 && is.null(best_cpdag_seen_so_far)) {
        best_cpdag_seen_so_far <- g2
        best_cpdag_seen_so_far_num_edges <- num_edges
        best_cpdag_seen_so_far_params <- result_line
      }

    }, error = function(e) {
      warning(sprintf("Failed iteration for PD=%.1f: %s", pd, conditionMessage(e)))
    })
  }

  # Bail out early if absolutely nothing passed any threshold
  if (!MC_passing_cpdag_already_found && is.null(cpdag_graph_when_PD_is_1)) {
    stop(structure(
      list(
        message = "No valid graph found; TetradSearch may have failed",
        call = match.call()
      ),
      class = c("simpleError", "error", "condition")
    ))
  }

  cat("\nThe best cpdag (the one with fewest edges among those for which unif > 0.1) has these attributes:\n")
  cat(headers_string, "\n")
  cat(best_cpdag_seen_so_far_params, "\n")

  graph <- if (!is.null(best_cpdag_seen_so_far)) {
    # A valid CPDAG passed Markov Check
    best_cpdag_seen_so_far
  } else {
    # Nothing passed, but fallback to something rather than crashing
    warning("Falling back to cpdag at PD=1.0, as no unif > 0.1 CPDAG was found")
    cpdag_graph_when_PD_is_1
  }

  graph_str <- tryCatch(
    .jcall(graph, "Ljava/lang/String;", "toString"),
    error = function(e) {
      warning(sprintf("Failed to convert graph to string: %s", conditionMessage(e)))
      NULL
    }
  )

  if (!is.null(graphtxt_buffer)) {
    if (is.null(graph_str) || !nzchar(graph_str)) {
      warning("graph_str is NULL or empty, not updating graphtxt_buffer.")
      graphtxt_buffer(NULL)
    } else {
      graphtxt_buffer(
        tryCatch(
          gsub("(?s)Graph Attributes:.*", "", graph_str, perl = TRUE),
          error = function(e) {
            warning(sprintf("Regex failed on graph_str: %s", conditionMessage(e)))
            graph_str  # fallback to raw string
          }
        )
      )
    }
  }

  return(list(graph, last_ts, MC_passing_cpdag_already_found, best_cpdag_seen_so_far))
}

AIR_getAdjSets <- function(ts, tv, ov, MC_passing_cpdag_already_found, best_cpdag_seen_so_far){
  TREATMENT_NAME = tv
  RESPONSE_NAME = ov
  MAX_NUM_SETS = 3
  MAX_DISTANCE_FROM_POINT = 4
  MAX_PATH_LENGTH = 4
  NEAR_TREATMENT = 1
  NEAR_RESPONSE = 2
  
  
  cat("Identification parameters: \n")
  cat("    maximum number of adjustment sets = ", MAX_NUM_SETS, "\n")  
  cat("    maximum distance from target endpoint (TREATMENT or RESPONSE) = ", MAX_DISTANCE_FROM_POINT, "\n")  
  cat("    maximum path length = ", MAX_PATH_LENGTH, "\n")  
  
  if (MC_passing_cpdag_already_found == TRUE) {
    cat("Searching for adjustment set(s) on the *** Treatment *** side:\n")
    # Z1
    adj_sets_treatment = ts$get_adjustment_sets(best_cpdag_seen_so_far, TREATMENT_NAME, RESPONSE_NAME,
                                                MAX_NUM_SETS,
                                                MAX_DISTANCE_FROM_POINT,
                                                NEAR_TREATMENT,
                                                MAX_PATH_LENGTH)
    
    ts$print_adjustment_sets(adj_sets_treatment)
    
    cat("Searching for adjustment sets on the *** Response *** side:\n")
    # Z2
    adj_sets_response  = ts$get_adjustment_sets(best_cpdag_seen_so_far, TREATMENT_NAME, RESPONSE_NAME,
                                                MAX_NUM_SETS,
                                                MAX_DISTANCE_FROM_POINT,
                                                NEAR_RESPONSE,
                                                MAX_PATH_LENGTH)
    
    ts$print_adjustment_sets(adj_sets_response)
  } 
  
  ## Initialize flag variables to indicate to AIR Step 3 that no/only one adjustment set is yet found
  flag_no_adjustment_set_found = FALSE
  flag_only_one_adjustment_set_found = FALSE
  
  # Determine the union and differences of the two adjustment sets
  union_of_two_lists = union(adj_sets_treatment, adj_sets_response)
  near_treatment_not_near_response = setdiff(adj_sets_treatment, adj_sets_response)
  near_response_not_near_treatment = setdiff(adj_sets_response, adj_sets_treatment)
  
  cat("Total number of adjustment sets encountered (ignoring duplicates) = ", length(union_of_two_lists), "\n")
  cat("Size of Treatment - Response adjustment sets = ", length(near_treatment_not_near_response), "\n")
  cat("Size of Response - Treatment adjustment sets = ", length(near_response_not_near_treatment), "\n")
  
  ## Now consider all cases where either set (or both sets) of adjustment sets is (are) empty.
  # if both empty, we need to set the corresponding flag:
  if (length(union_of_two_lists) == 0) {
    # Then no adjustment sets and we must be working with a cpdag (at least one undirected
    #   edge) rather than a DAG. (For a DAG, there's always an adjustment set--namely the
    #   set of parents of the treatment variable, which can be empty, but that's still
    #   a valid adjustment set.) There are multiple solutions here, but get the end-user
    #   involved.
    cat("*** No adjustment set found. Either: ")
    cat("Revise knowledge (see AIR job aid) so search result has no undirected edges. ***\n")
    flag_no_adjustment_set_found = TRUE    
  } else if (length(union_of_two_lists)==1) {
    # Only one adjustment set is found altogether, and so we set the corresponding flag:
    flag_only_one_adjustment_set_found = TRUE
    return_first_adj_set  = union_of_two_lists[[1]]
    return_second_adj_set = union_of_two_lists[[1]]  # same adjustment set
  } else {
    # We have at least two distinct adjustment sets; we prefer ones only from each side if practical, so, test alternatives first.
    cat("At least two adjustment sets found. \n")
    
    # if no adjustment set found near response that was not already near treatment:
    if (length(near_response_not_near_treatment) == 0) {
      cat("In this case, there is no adjustment set near response that is not also near treatment.\n")
      return_first_adj_set  = adj_sets_treatment[[1]]
      return_second_adj_set = adj_sets_treatment[[2]]
    } else if (length(near_treatment_not_near_response) == 0) {
      cat("In this case, there is no adjustment set near treatment that is not also near response.\n")
      return_first_adj_set  = adj_sets_response[[1]]
      return_second_adj_set = adj_sets_response[[2]]
    } else {
      # At least one adjustment set is near treatment but not response, and vice versa.
      #   This is the ideal case to obtain greater diversity of adjustment sets, which 
      #   is also why we might want to set max_num_sets higher.
      #   Return two distinct adjustment sets, one from each side.
      cat("In this case, we have found at least one adjustment set near treatment but not near response; and vice versa.\n")
      return_first_adj_set  = near_treatment_not_near_response[[1]]
      return_second_adj_set = near_response_not_near_treatment[[1]]
    }
  }
  
  cat("Summary of results: \n")
  cat("First adjustment set to return: ", return_first_adj_set, "\n")
  cat("Second adjustment set to return: ", return_second_adj_set, "\n")
  cat("Flag status for no adjustment set found: ", flag_no_adjustment_set_found, "\n")
  cat("Flag status for only one adjustment set found: ", flag_only_one_adjustment_set_found, "\n")
  
  ### return the two adjustment sets plus the two flags instead.
  return(list(return_first_adj_set, return_second_adj_set))
}

scale_ <- function(x){
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}

runSuperLearner <- function(settings, AIRHome,
                            data_df,
                            tv_dir, tv_threshold,
                            ov_dir, ov_threshold,
                            log_file){ 
  cat(paste0(Sys.time(), " - ","Started superlearner with ",
             paste(c(settings, AIRHome, tv_dir, tv_threshold, ov_dir, ov_threshold), collapse = ", ")), "\n", 
      file = log_file, 
      append = TRUE)
  Z_level <- settings$Z_level
  doc_title <- settings$doc_title
  
  cat("Reading in data\n", file = log_file, append = TRUE)
  confounders <- as.character(strsplit(x = settings$confounders, split = " ")[[1]])
  treatment <- as.character(settings$varName)
  outcome =  as.character(df_vars[df_vars$var == "OV",]$val)

  df <- data_df
  if (tv_dir == ">") {
    df[[treatment]] <- ifelse(df[[treatment]] > tv_threshold, 1, 0)
  } else if (tv_dir == ">=") {
    df[[treatment]] <- ifelse(df[[treatment]] >= tv_threshold, 1, 0)
  } else if (tv_dir == "<") {
    df[[treatment]] <- ifelse(df[[treatment]] < tv_threshold, 1, 0)
  } else if (tv_dir == "<=") {
    df[[treatment]] <- ifelse(df[[treatment]] <= tv_threshold, 1, 0)
  } else if (tv_dir == "=") {
    df[[treatment]] <- ifelse(df[[treatment]] == tv_threshold, 1, 0)
  } 
  if (ov_dir == ">") {
    df[[outcome]] <- ifelse(df[[outcome]] > ov_threshold, 1, 0)
  } else if (ov_dir == ">=") {
    df[[outcome]] <- ifelse(df[[outcome]] >= ov_threshold, 1, 0)
  } else if (ov_dir == "<") {
    df[[outcome]] <- ifelse(df[[outcome]] < ov_threshold, 1, 0)
  } else if (ov_dir == "<=") {
    df[[outcome]] <- ifelse(df[[outcome]] <= ov_threshold, 1, 0)
  } else if (ov_dir == "=") {
    df[[outcome]] <- ifelse(df[[outcome]] == ov_threshold, 1, 0)
  } 

  ## ────────────────────────────────────────────────────────────────
  ##  Coerce the *outcome* to numeric if it’s still a factor
  ##  (TMLE + sl3 learners expect a numeric Y for a “continuous”
  ##   outcome; leaving it as factor triggers the warning you saw)
  ## ────────────────────────────────────────────────────────────────
  if (is.factor(df[[outcome]])) {
    warning(sprintf(
      "Outcome %s is a factor – coercing to numeric {0,1}.",
      outcome
    ), call. = FALSE)

    ## factor levels are usually 1 / 2 after the binarisation above,
    ## so subtract 1 to make them {0,1}
    df[[outcome]] <- as.numeric(df[[outcome]]) - 1
  }

  #### TMLE -------------------------------------
  ##### Define Superlearner -------------------
  # sl3_list_learners("binomial") 
  
  cat("Building learner list\n", file = log_file, append = TRUE)
  lrnr_mean <- sl3::make_learner(sl3::Lrnr_mean)
  lrnr_glm <- sl3::make_learner(sl3::Lrnr_glm)
  lrnr_hal <- sl3::make_learner(sl3::Lrnr_hal9001)
  lrnr_nnet <- sl3::make_learner(sl3::Lrnr_nnet)

  ## sl3 ≥ 0.3.0:  Lrnr_ranger            (C++ backend, seed-respecting)
  ## sl3 ≤ 0.2.x:  Lrnr_randomForest      (R backend)
  lrnr_rf  <- if ("Lrnr_ranger" %in% getNamespaceExports("sl3"))
                sl3::make_learner(sl3::Lrnr_ranger)
              else
                sl3::make_learner(sl3::Lrnr_randomForest)

  lrnr_glmnet <- sl3::make_learner(sl3::Lrnr_glmnet)
  lrnr_xgboost <- sl3::make_learner(sl3::Lrnr_xgboost, max_depth = 4, eta = 0.01, nrounds = 100)
  lrnr_earth <- sl3::make_learner(sl3::Lrnr_earth)
  if (length(confounders) > 1) {
    sl_ <- sl3::make_learner(sl3::Stack, unlist(list(lrnr_mean, 
                                           lrnr_glm,
                                           lrnr_hal,
                                           lrnr_rf,
                                           lrnr_glmnet,
                                           lrnr_xgboost,
                                           lrnr_earth,
                                           lrnr_nnet),
                                      recursive = TRUE))
  } else {
    sl_ <- sl3::make_learner(sl3::Stack, unlist(list(lrnr_mean, 
                                           lrnr_glm,
                                           lrnr_hal,
                                           lrnr_rf,
                                           lrnr_xgboost,
                                           lrnr_earth,
                                           lrnr_nnet), 
                                      recursive = TRUE))
  }
  # DEFINE SL_Y AND SL_A 
  # We only need one, because they're the same
  ##### Define Formulae --------------------------
  Q_learner <- sl3::Lrnr_sl$new(learners = sl_, 
                           metalearner = sl3::Lrnr_nnls$new(convex = T)) # output model
  g_learner <- sl3::Lrnr_sl$new(learners = sl_, 
                           metalearner = sl3::Lrnr_nnls$new(convex = T)) # treatment model
  learner_list <- list(Y = Q_learner,
                       A = g_learner)
  
  # PREPARE THE THINGS WE WANT TO FEED IN TO TMLE3
  ate_spec <- tmle3::tmle_ATE(treatment_level = 1, control_level = 0)
  

  
  ##### Nodes ------------------
  nodes_ <- list(W = confounders, # covariates
                 A = treatment,
                 # Z = mediators, # unnecessary unless doing mediation analysis
                 Y = outcome)
  
  ##### RUN TMLE3 -------------------------------
  set.seed(123)
  ### this is where the parallel is breaking
  cat("Starting TMLE\n", file = log_file, append = TRUE)
  tryCatch({
    tmle_fit_ <- tmle3::tmle3(tmle_spec = ate_spec,
                 data = df,
                 node_list = nodes_,
                 learner_list = learner_list)
  }, error = function(e) {
    cat(sprintf("Error in tmle3 call: %s\n", conditionMessage(e)), 
        file = log_file, append = TRUE)
    stop(e)
  })
  cat("Pulling out TMLE scores\n", file = log_file, append = TRUE)
  tmle_task <- ate_spec$make_tmle_task(df, nodes_)
  
  initial_likelihood <- ate_spec$make_initial_likelihood(
    tmle_task,
    learner_list
  )
  
  ## save propensity score for diagnosis
  propensity_score <- initial_likelihood$get_likelihoods(tmle_task)$A
  propensity_score <- propensity_score * df[,..treatment] + (1 - propensity_score) * (1 - df[,..treatment])
  
  plap_ <- tibble(exposure = df[,..treatment] |> pull(),
                  pscore = propensity_score |> pull())
  
  plap_$sw <- plap_$exposure * (mean(plap_$exposure)/propensity_score) + (1 - plap_$exposure) * ((1 - mean(plap_$exposure)) / (1 - propensity_score))

  ## inside runSuperLearner() – after tmle_fit_ is available
  prefix <- tolower(Z_level)    # "z1"  or  "z2"

  ## Extract the three CI numbers *as vectors*
  est  <- as.numeric(tmle_fit_$summary[[8]])
  lci  <- as.numeric(tmle_fit_$summary[[9]])
  uci  <- as.numeric(tmle_fit_$summary[[10]])

  tmle_row <- tibble::tibble(
    Treatment = treatment,
    Group     = Z_level,
  
    ## fill the columns that belong to *this* adjustment set …
    z1_Mean = if (prefix == "z1") est else NA_real_,
    z1_LCI  = if (prefix == "z1") lci else NA_real_,
    z1_UCI  = if (prefix == "z1") uci else NA_real_,
  
    z2_Mean = if (prefix == "z2") est else NA_real_,
    z2_LCI  = if (prefix == "z2") lci else NA_real_,
    z2_UCI  = if (prefix == "z2") uci else NA_real_
  )

  # save outcome predictions for diagnosis
  # initial_likelihood_preds was formerly labeled outcome_preds
  initial_likelihood_preds <- initial_likelihood$get_likelihoods(tmle_task,"Y")
  # define and fit likelihood
  factor_list <- list(
    tmle3::define_lf(LF_emp, "W"),
    tmle3::define_lf(LF_fit, "A", sl_),
    tmle3::define_lf(LF_fit, "Y", sl_, type = "mean")
  )
  likelihood_def <- tmle3::Likelihood$new(factor_list)
  likelihood <- likelihood_def$train(tmle_task)
  likelihood_values <- rowMeans(likelihood$get_likelihoods(tmle_task,"Y"))
  
  # print("super learner coefficients for PS model")
  g_fit <- tmle_fit_$likelihood$factor_list[["A"]]$learner
  # g_fit$fit_object$full_fit$learner_fits$Lrnr_nnls_TRUE
  
  # print("super learner coefficients for outcome model")
  Q_fit <- tmle_fit_$likelihood$factor_list[["Y"]]$learner
  # Q_fit$fit_object$full_fit$learner_fits$Lrnr_nnls_TRUE
  
  ## generate counterfactuals
  ### counterfactual where all treatments set to 1
  intervention1 <- tmle3::define_lf(LF_static, "A", value = 1)
  
  cf_likelihood1 <- tmle3::make_CF_Likelihood(likelihood, intervention1)
  
  cf_likelihood_values1 <- cf_likelihood1$get_likelihoods(tmle_task, "A")
  
  # We can then use this to construct a counterfactual likelihood:
  ### counterfactual where all treatments set to 0
  # set values
  intervention0 <- tmle3::define_lf(LF_static, "A", value = 0)
  # generate counterfactual likelihood object
  cf_likelihood0 <- tmle3::make_CF_Likelihood(likelihood, intervention0)
  # get likelihoods from object
  cf_likelihood_values0 <- cf_likelihood0$get_likelihoods(tmle_task, "A")
  # We see that the likelihood values for the A node are all either 0 or 1, as would be expected from an indicator likelihood function. In addition, the likelihood values for the non-intervention nodes have not changed.
  cat("Building Output\n", file = log_file, append = TRUE)
  ## output individual row values
  # df_out <- df[,c(nodes_$A, nodes_$Y, nodes_$W)]
  df_out <- df |> select(nodes_$A, nodes_$Y, nodes_$W)
  df_out$exposure <- plap_$exposure
  df_out$rownum <- rownames(df_out)
  df_out$pscore <- plap_$pscore
  df_out$sw <- plap_$sw
  df_out$tmle_est <- tmle_fit_$estimates[[1]]$IC
  df_out$initial_likelihood_preds <- initial_likelihood_preds
  df_out$likelihood_values <- likelihood_values
  df_out$counterfactual_0 <- cf_likelihood_values0
  df_out$counterfactual_1 <- cf_likelihood_values1
  df_out$g_fit_pred <- g_fit$predict() 
  df_out$Q_fit_pred <- Q_fit$predict()
  cat("Finished SuperLearner – returning results\n", file = log_file, append = TRUE)

  # -------- FINAL RETURN (nothing after this executes) ------------
  list(
    tmle = tmle_row,                                # one‐row summary
    diag = list(doc_title = doc_title, data = df_out)  # diagnostics
  )
}

processResults <- function(settings, AIRHome, tv_dir, tv_threshold, ov_dir, ov_threshold, model_in, model_yn, model_ate, log_file, move_results = FALSE) {
  log_msg <- function(...) {
    cat(paste0(Sys.time(), " - ", ...), "\n", file = log_file, append = TRUE)
  }

  log_msg("Started processResults()")
  log_msg("Settings:", paste(c(settings, AIRHome, tv_dir, tv_threshold, ov_dir, ov_threshold, model_yn, model_ate), collapse = ", "))

  treatment <- as.character(settings$varName)
  outcome <- as.character(df_vars[df_vars$var == "OV", ]$val)
  confounders <- as.character(unique(Zvars$Z))
  doc_title <- settings$doc_title

  df <- req(datafile_buffer())
  set.seed(123)

  # Bin conversion logic
  binarize_var <- function(df, var, dir, threshold) {
    if (dir == ">") df[[var]] <- ifelse(df[[var]] > threshold, 1, 0)
    else if (dir == ">=") df[[var]] <- ifelse(df[[var]] >= threshold, 1, 0)
    else if (dir == "<") df[[var]] <- ifelse(df[[var]] < threshold, 1, 0)
    else if (dir == "<=") df[[var]] <- ifelse(df[[var]] <= threshold, 1, 0)
    else if (dir == "=") df[[var]] <- ifelse(df[[var]] == threshold, 1, 0)
    return(df)
  }

  df <- binarize_var(df, treatment, tv_dir, tv_threshold)
  df <- binarize_var(df, outcome, ov_dir, ov_threshold)

  # Split into train/test
  test_size <- floor(0.3 * nrow(df))
  samp <- sample(nrow(df), test_size)
  ## ML learners downstream need a *numeric* 0/1 response.  Coerce once here
  ## and keep the original column name.
  y_train <- df[-samp, outcome, drop = FALSE] |>
             mutate(!!outcome := as.numeric(.data[[outcome]]))
  x_train <- df[-samp, setdiff(names(df), outcome), drop = FALSE]
  y_test  <- df[samp,  outcome, drop = FALSE] |>
             mutate(!!outcome := as.numeric(.data[[outcome]]))
  x_test <- df[samp, setdiff(names(df), outcome), drop = FALSE]
  train <- cbind(label = y_train[[1]], x_train)
  test <- cbind(label = y_test[[1]], x_test)
  colnames(train)[1] <- "label"
  colnames(test)[1] <- "label"
  xtest_0 <- mutate(x_test, !!treatment := 0)
  xtest_1 <- mutate(x_test, !!treatment := 1)

  log_msg("Data loaded and split.")

  if (model_yn == "No") {
    log_msg("Running new ML models")

    ## --- helper: a “safe” GLM that gracefully falls back to lm() -------------
    safe_glm <- function(formula, data, ...) {
      tryCatch(
        {
          suppressWarnings(                     # keep harmless warnings quiet
            glm(formula, data = data, family = binomial, ...)
          )
        },
        warning = function(w) {
          log_msg(sprintf(
            "⚠️  glm(): %s – falling back to linear regression.", 
            conditionMessage(w)
          ))
          lm(formula, data = data)              # fall back → numeric preds
        },
        error = function(e) {
          log_msg(sprintf(
            "❌  glm() failed: %s – using lm() instead.", 
            conditionMessage(e)
          ))
          lm(formula, data = data)
        }
      )
    }

    ## all four learners now see a numeric 0/1 outcome
    model_lm  <- safe_glm(label ~ ., data = train)
    ## --- helper: run rpart in "class" mode when the response is 0/1 ----------
    safe_rpart <- function(formula, data, ...) {
      y <- data[[all.vars(formula)[1L]]]
      is_binary <- is.numeric(y) && setequal(unique(y), c(0, 1))
      method    <- if (is_binary) "class" else "anova"

      tryCatch(
        {
          suppressWarnings(
            rpart::rpart(formula, data = data, method = method, ...)
          )
        },
        warning = function(w) {
          log_msg(sprintf(
            "⚠️  rpart(): %s – continuing with produced tree.", 
            conditionMessage(w)
          ))
          invokeRestart("muffleWarning")        # keep going
        }
      )
    }

    ## --- helper: pick the right SVM mode & silence benign warnings --------
    safe_svm <- function(formula, data, ...) {
      y <- data[[all.vars(formula)[1L]]]
      is_binary <- is.numeric(y) && setequal(unique(y), c(0, 1))
      svm_type  <- if (is_binary) "C-classification" else "eps-regression"

      tryCatch(
        {
          suppressWarnings(
            e1071::svm(formula,
                       data = data,
                       type = svm_type,
                       ...)
          )
        },
        warning = function(w) {
          log_msg(sprintf(
            "⚠️  svm(): %s – continuing with produced model.",
            conditionMessage(w)
          ))
          invokeRestart("muffleWarning")
        }
      )
    }

    model_dt  <- safe_rpart(label ~ ., data = train)               # tree
    model_svm <- safe_svm(label ~ ., data = train)                 # SVM
    model_rf  <- randomForest(label ~ ., data = train)

    pred_lm0 <- predict(model_lm, xtest_0)
    pred_lm1 <- predict(model_lm, xtest_1)
    pred_dt0 <- predict(model_dt, xtest_0)
    pred_dt1 <- predict(model_dt, xtest_1)

    ## SVM: use probabilities when we trained a classifier (otherwise numeric)
    if (model_svm$type == "C-classification") {
      pred_svm0 <- attr(predict(model_svm, xtest_0, probability = TRUE),
                        "probabilities")[, "yes"]
      pred_svm1 <- attr(predict(model_svm, xtest_1, probability = TRUE),
                        "probabilities")[, "yes"]
    } else {
      pred_svm0 <- as.numeric(predict(model_svm, xtest_0))
      pred_svm1 <- as.numeric(predict(model_svm, xtest_1))
    }
    pred_rf0 <- predict(model_rf, xtest_0)
    pred_rf1 <- predict(model_rf, xtest_1)

    lm_ate <- mean(pred_lm1) - mean(pred_lm0)
    dt_ate <- mean(pred_dt1) - mean(pred_dt0)
    svm_ate <- mean(as.numeric(pred_svm1)) - mean(as.numeric(pred_svm0))
    rf_ate <- mean(as.numeric(pred_rf1)) - mean(as.numeric(pred_rf0))

    covariates <- setdiff(names(train), "label")
    ## choose the proper outcome_type ------------------------------------------
    out_type <- if (is.numeric(train$label) &&
                    setequal(unique(train$label), c(0, 1)))
                  "binomial" else "continuous"

    task <- sl3::make_sl3_Task(data         = train,
                               outcome      = "label",
                               covariates   = covariates,
                               outcome_type = out_type)
    learners <- list(
      sl3::Lrnr_glm$new(),
      sl3::Lrnr_glmnet$new(),
      sl3::Lrnr_xgboost$new(),
      sl3::Lrnr_earth$new(),
      sl3::Lrnr_nnet$new(),
      sl3::Lrnr_svm$new()
    )
    sl <- sl3::Lrnr_sl$new(
      learners     = sl3::Stack$new(learners),
      metalearner  = sl3::Lrnr_nnls$new()
    )
    sl_fit <- sl$train(task)

    ## -- helper: always hand rowMeans() a 2-D object ------------------------
    make_pred <- function(task) {
      p <- sl_fit$predict(task)
      if (is.null(dim(p)))               # <- only one learner returned
        p <- matrix(p, ncol = 1)         #    promote to 1-col matrix
      rowMeans(p, na.rm = TRUE)
    }

    pred_sl0 <- make_pred(sl3::make_sl3_Task(data = xtest_0,
                                             covariates = covariates))
    pred_sl1 <- make_pred(sl3::make_sl3_Task(data = xtest_1,
                                             covariates = covariates))
    sl_ate <- mean(pred_sl1) - mean(pred_sl0)

    log_msg("Model training complete")

  ate_df <- data.frame(
    algorithm = c("Logistic Regression", "Decision Tree", "Support Vector Machine",
                  "Random Forest", "Stacked Super Learner"),
    flag      = c(lm_ate, dt_ate, svm_ate, rf_ate, sl_ate),
    ## give every row the variable name that the model is estimating
    ## (vector recycling takes care of the length-mismatch)
    Treatment = outcome
  )

  ## stash them – ONE row per algorithm
  ml_buffer(ate_df)

  } else {
    log_msg(sprintf("Using existing model config: %s", model_yn))

    if (model_yn == "Yes") {
      if (is.null(model_in)) {
        log_msg("⚠️  No model file was supplied – skipping custom-model ATE.")
        return(invisible(NULL))            # bail out, nothing downstream to do
      }
  
      pred_m0 <- predict(model_in, xtest_0)
      pred_m1 <- predict(model_in, xtest_1)
      m_ate   <- mean(as.numeric(pred_m1)) - mean(as.numeric(pred_m0))
    } else if (model_yn == "ATE") {
      m_ate <- model_ate
    }

    ate_df <- data.frame(
      algorithm = "Existing Model",
      flag      = m_ate,
      Treatment = outcome
    )

    ml_buffer(ate_df)
  }

  # ---------------------------------------------------------------------------
  # small helpers
  # ---------------------------------------------------------------------------
  safe_drop <- function(.data, cols) {
    dplyr::select(.data, -dplyr::any_of(cols))
  }
  
  force_numeric_cols <- function(.data, cols) {
    missing <- setdiff(cols, names(.data))
    if (length(missing) > 0) .data[missing] <- NA_real_
    dplyr::mutate(.data, dplyr::across(all_of(cols), as.numeric))
  }
  
  # ---------------------------------------------------------------------------
  # 1. pull whatever the workers have written so far --------------------------
  # ---------------------------------------------------------------------------
  results <- results_buffer() |> tibble::as_tibble()
  
  ## ── NEW: bail out if *all* learners failed ─────────────────────────────
  if (nrow(results) == 0 ||
      purrr::every(
        results[c("z1_Mean","z1_LCI","z1_UCI",
                  "z2_Mean","z2_LCI","z2_UCI")],
        ~ all(is.na(.x))
      )) {
    log_msg("❌ All TMLE calls returned NA — nothing to summarise.")
    shiny::showNotification(
      "All adjustment-set learners failed – no ATEs to display.",
      type = "error", duration = 8
    )
    return(invisible(NULL))   # abort before any plotting math
  }
  
  # ---------------------------------------------------------------------------
  # 2. schema guards – be SURE the numeric columns are really numeric ----------
  # ---------------------------------------------------------------------------
  if (!"Group" %in% names(results)) results <- dplyr::mutate(results, Group = "z")
  
  # keep Treatment as character; coerce everything else (except Group)
  results <- results |>
    dplyr::mutate(
      Treatment = as.character(Treatment),
      dplyr::across(-c(Group, Treatment), ~ as.numeric(unlist(.x)))
    )
  
  log_msg(
    "results snapshot >>>\n",
    paste(
      utils::capture.output({
        print(utils::head(results, 6))
        str(results)
      }),
      collapse = "\n"
    ),
    "\n<<< end snapshot"
  )
  
  # ---------------------------------------------------------------------------
  # 3. (A) build the *causal* results frame – **ML stays outside**
  # ---------------------------------------------------------------------------
  causal_df <- results |>                       # **only TMLE rows here**
    dplyr::mutate(
      Treatment = outcome %||% Treatment        # ensure present / non-NA
    ) |>
    ## rename ONLY *after* the sig-test so those columns still exist
    dplyr::rename(
      z1_ATE     = z1_Mean,
      z1_ATE_LCI = z1_LCI,
      z1_ATE_UCI = z1_UCI,
      z2_ATE     = z2_Mean,
      z2_ATE_LCI = z2_LCI,
      z2_ATE_UCI = z2_UCI
    )

  # ---------------------------------------------------------------------------
  # 3. (B) bind in the *ML* ATEs and derive significance flags
  #       – this is now a clean join of two equally-shaped tables
  # ---------------------------------------------------------------------------
  ml_df <- ml_buffer()                           # <- comes from runSuperLearner()
  
  final_results <- dplyr::full_join(causal_df, ml_df,
                                    by = "Treatment") |>
    dplyr::mutate(
      z1_sig = ifelse(
        !is.na(z1_ATE_LCI) & !is.na(z1_ATE_UCI) &
        (flag < z1_ATE_LCI | flag > z1_ATE_UCI),
        1, 0
      ),
      z2_sig = ifelse(
        !is.na(z2_ATE_LCI) & !is.na(z2_ATE_UCI) &
        (flag < z2_ATE_LCI | flag > z2_ATE_UCI),
        1, 0
      ),
      significance = pmax(z1_sig, z2_sig, na.rm = TRUE)
    )

  ## ── NEW: extra guard & verbose logging ──────────────────────────────
  if (!"Treatment" %in% names(final_results)) {
    log_msg("⚠️  Treatment column absent – creating it from settings$varName.")
    final_results <- dplyr::mutate(final_results,
                                   Treatment = settings$varName)
  } else {
    final_results <- dplyr::mutate(final_results,
                                   Treatment = tidyr::replace_na(Treatment,
                                                                 settings$varName))
  }

  # crash loudly *here* (where the problem is easy to see) if anything
  # critical is still NA/NULL
  req_cols <- c("Treatment", "z1_ATE_LCI", "z1_ATE_UCI",
                "z2_ATE_LCI", "z2_ATE_UCI")
  missing_vals <- purrr::keep(final_results[req_cols], ~ all(is.na(.x)))
  if (length(missing_vals) > 0) {
    stop("processResults(): critical columns still NA – see log for details")
  }

  # log a concise snapshot for post-mortem debugging
  log_msg(
    "final_results snapshot →\n",
    paste(
      utils::capture.output({
        print(utils::head(final_results, 6))
        str(final_results)
      }),
      collapse = "\n"
    )
  )
  
  # ---------------------------------------------------------------------------
  # 4. provide a one-row "combined" summary the UI helpers expect -------------
  # ---------------------------------------------------------------------------
  combined_row <- final_results |>
    summarise(across(
      c(z1_ATE, z1_ATE_LCI, z1_ATE_UCI,
        z2_ATE, z2_ATE_LCI, z2_ATE_UCI,
        flag, z1_sig, z2_sig, significance),
      ~ first(na.omit(.x))
    )) |>
    mutate(Group = "combined",
           Treatment = first(final_results$Treatment),
           algorithm = first(final_results$algorithm))
  
  # ---------------------------------------------------------------------------
  # 5. stash both flavours for later use --------------------------------------
  #    • the detailed two-row frame (if you need it somewhere else)
  #    • the one-row “combined” frame the plots/text expect
  # ---------------------------------------------------------------------------
  results_out_buffer(combined_row)
  assign("results_detailed", final_results, envir = .GlobalEnv)   # <<- optional
  
  log_msg("Results processed and stored.")
}

## Create four simple methods to help parse a line

# Is the first character of a line a nonzero digit? If yes, then that line specifies an edge.
is_edge_line <- function(line) {
  # regular expression
  return(grepl("^[1-9]", line))
}

# Split a string of space-delimited substrings into a list of those substrings
split_line <- function(input_string) {
  return(strsplit(input_string, " "))
}

# Return the first node from a line specifying an edge (after the line number)
first_node <- function(line) {
  line_components <- split_line(line)
  return(line_components[[1]][2])
}
# Return the second node from a line specifying an edge (after the line number)
second_node <- function(line) {
  line_components <- split_line(line)
  return(line_components[[1]][4])
}

## The key methods defined here are descendants (and its dual: ancestors)
descendants <- function(node, children) {
  checked_so_far <- set()
  seen_not_checked <- set(node)
  while (!set_is_empty(seen_not_checked)) {
    check_these <- seen_not_checked
    for (n in check_these) {
      for (m in children[[n]]) {
        if ( !(m %e% checked_so_far)) {
          seen_not_checked <- seen_not_checked | set(m)
        }
      }
      seen_not_checked <- seen_not_checked - set(n)
      checked_so_far <- checked_so_far | set(n)
    }
  }
  return(checked_so_far)
}

get_X_descendents <- function(TV, graphtxt_buffer) {
  lines <- strsplit(graphtxt_buffer(), "\n")[[1]]
  
  nodes <- set()
  for (line in lines) {
    if (is_edge_line(line)) {
      if (!(first_node(line)  %e% nodes)) {
        nodes <- nodes | set(first_node(line))
      }
      if (!(second_node(line) %e% nodes)) {
        nodes <- nodes | set(second_node(line))
      }
    }
  }

  ## Create children and parents dictionaries
  children <- hash()
  for (n in nodes) {
    children_of_n <- set()
    for (line in lines) {
      if (is_edge_line(line)) {
        if ((first_node(line)  == n) && (!(second_node(line) %e% children_of_n))) {
          children_of_n <- children_of_n | set(second_node(line))
        }
      }
    }
    children[[n]] <- children_of_n
  }
  return(unlist(descendants(TV, children)))
}


get_ribbon_plot <- function(AIRHome) {
  dfr <- results_out_buffer()

  if (nrow(dfr) == 0) {
    return(tags$em("No results – model fitting failed. Check the log."))
  }

  nz_max <- function(x) if (all(is.na(x))) NA_real_ else max(x, na.rm = TRUE)
  nz_min <- function(x) if (all(is.na(x))) NA_real_ else min(x, na.rm = TRUE)

  # safety check:
  #  * use new column names after the earlier rename()
  if (nrow(dfr) == 0 || all(is.na(dfr$z1_ATE)) ) {
    return(tags$div("No causal estimates available for this run."))
  }
  
  # derive *safe* CI bounds up-front (no ±Inf, no NA explosions)
  lo_all     <- nz_min(c(dfr$z1_ATE_LCI, dfr$z2_ATE_LCI))
  hi_all     <- nz_max(c(dfr$z1_ATE_UCI, dfr$z2_ATE_UCI))
  lo_overlap <- nz_max(c(dfr$z1_ATE_LCI, dfr$z2_ATE_LCI))
  hi_overlap <- nz_min(c(dfr$z1_ATE_UCI, dfr$z2_ATE_UCI))

  # If *everything* is NA we can’t draw a ribbon — tell the user and bail out
  if (is.na(lo_all) || is.na(hi_all)) {
    shiny::showNotification(
      "Both confidence intervals are NA – nothing to plot.",
      type = "warning", duration = 8
    )
    return(tags$div("No confidence intervals to plot."))
  }

  # code for generating ribbon plot
  if (any(dfr$flag >= dfr$z1_ATE_LCI & dfr$flag <= dfr$z1_ATE_UCI)) {
    inZ1 <- TRUE
  } else { inZ1 <- FALSE }
  if (any(dfr$flag >= dfr$z2_ATE_LCI & dfr$flag <= dfr$z2_ATE_UCI)) {
    inZ2 <- TRUE
  } else { inZ2 <- FALSE }
  
  summary_color <- case_when(
    inZ1 == TRUE & inZ2 == TRUE ~ "#378855",
    inZ1 == TRUE | inZ2 == TRUE ~ "#FCB514",
    inZ1 == FALSE & inZ2 == FALSE ~ "#C00000"
  )
  
  dfr0 <- dfr[1,]
  p <- ggplot(dfr0, aes(x = Treatment)) +

    ## background
    geom_linerange(aes(ymin = -1.05, ymax = 1.05),
                   lwd = 6,
                   col = "black",
                   alpha = 1,
                   stat = "unique",
                   lineend = "round",
                   position = position_nudge(x = 0)) +
    ## Annotations for '-' and '+'
    # annotate("text", x = 0.92, y = -1.07, label = "-", hjust = 0, vjust = 0, size = 5, color = "white") +
    # annotate("text", x = 0.88, y = 1.025,  label = "+", hjust = 0, vjust = 0, size = 5, color = "white") +
    annotate("text", 
             x = 1,  # Position on the left side within the black background
             y = -1,  # Center vertically within the black background
             label = "-", 
             hjust = 2.5, 
             vjust = 0.25, 
             size = 5, 
             color = "white") +
    annotate("text", 
             x = 1,  # Position on the right side within the black background (adjust based on x-axis limits)
             y = 1,  # Center vertically within the black background
             label = "+", 
             hjust = -0.75, 
             vjust = 0.4, 
             size = 5, 
             color = "white") +## algorithm estimates
    ## Z1
    geom_linerange(aes(ymin = z1_ATE_LCI, ymax = z1_ATE_UCI),
                   lwd = 3.5,
                   col = "#9394A2",
                   alpha = 1,
                   stat = "unique",
                   lineend = "round",
                   position = position_nudge(x = 1)) +
    geom_point(aes(y = z1_ATE),
               col = "white",
               cex = 3,
               pch = 1,
               stroke = 1.25,
               position = position_nudge(x = 1)) +
    ## Z2
    geom_linerange(aes(ymin = z2_ATE_LCI, ymax = z2_ATE_UCI),
                   lwd = 3.5,
                   col = "#D4C7C7",
                   alpha = 1,
                   stat = "unique",
                   lineend = "round",
                   position = position_nudge(x = 0.5)) +
    geom_point(aes(y = z2_ATE),
               col = "white",
               cex = 3,
               pch = 1,
               stroke = 1.25,
               position = position_nudge(x = 0.5)) +
    # creating the ribbon
    geom_linerange(aes(ymin = -1, ymax = 1),
                   lwd = 3.5,
                   col = "#C00000",
                   alpha = 1,
                   stat = "unique",
                   lineend = "round",
                   position = position_nudge(x = 0)) +

                   # full (yellow) ribbon: min of all LCIs → max of all UCIs
                   geom_linerange(aes(ymin = lo_all, ymax = hi_all),

                   lwd = 3.5,
                   col = "#FCB514",
                   alpha = 1,
                   stat = "unique",
                   lineend = "round",
                   position = position_nudge(x = 0)) +

                   # overlap (green) ribbon: max LCI → min UCI
                   geom_linerange(aes(ymin = lo_overlap, ymax = hi_overlap),

                   lwd = 3.5,
                   col = "#378855",
                   alpha = 1,
                   stat = "unique",
                   lineend = "round",
                   position = position_nudge(x = 0)) +
    geom_segment(aes(x = 0.6, xend = 1.35, y = 0, yend = 0), lwd = 1.2) +
    ## algorithm estimates
    labs(y = "",
         x = "") +
    geom_segment(data = dfr,
                 aes(x = 2.5, xend = 1.25, y = flag, yend = flag, color = algorithm),
                 arrow = arrow(length = unit(0.5, "cm")),
                 lwd = 1.2,
                 color = "#0F9ED5") +
    geom_point(data = dfr,
               aes(x = 2.5, y = flag, shape = algorithm),
               size = 3,  # Adjust size as needed
               color = "#0F9ED5") +  # Or any desired color
    coord_flip(clip = "off") +
    ## Adjust Scales to Remove Expansion and Compress Vertically
    scale_x_discrete(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0),
                       limits = c(-1.1, 1.1)) +  # Tighten y-axis limits
    ## Theme Adjustments to Minimize White Space
    theme_void(base_size = 10) +
    # theme(
    #   # panel.spacing = unit(0, "pt"),
    #   panel.background = element_rect(fill = "transparent", color = NA),
    #   plot.background = element_rect(fill = "transparent", color = NA),
    #   aspect.ratio = 0.2
    # )
    theme(
      panel.background = element_rect(fill = "transparent", color = NA),
      plot.background = element_rect(fill = "transparent", color = NA),
      aspect.ratio = 0.2
    )
}

get_figure_caption <- function(AIRHome, df_vars) {
  caption <- paste0("Risk Difference: This chart represents the difference in outcomes resulting from a change in your experimental variable,",df_vars[1,][[2]],". The x-axis ranges from negative to positive effect, where the treatment, ", df_vars[2,][[2]]," either increases the likelihood of the outcome or decreases it, respectively. The midpoint corresponds to 'no significant effect.")
  return(caption)
}

get_ui_interpretation <- function(AIRHome, df_vars, Zvars) {
  dfr <- results_out_buffer()

  if (nrow(dfr) == 0) {
    return(tags$em("No results – model fitting failed. Check the log."))
  }

  interpretation <- "What we can learn from these results"
  dfr$zmax <- max(dfr$z1_ATE_UCI, dfr$z2_ATE_UCI)
  dfr$zmin <- min(dfr$z1_ATE_LCI, dfr$z2_ATE_LCI)
  
  if ((all(dfr$z1_ATE_UCI < dfr$z2_ATE) & all(dfr$z2_ATE_LCI > dfr$z1_ATE)) |
      (all(dfr$z1_ATE_LCI > dfr$z2_ATE) & all(dfr$z2_ATE_UCI < dfr$z1_ATE))) {
    interpretation <- "Inconsistent Causal ATE suggests not enough information to properly train a model."
  } else if (all(dfr$flag > dfr$zmin & dfr$flag < dfr$zmax)) {
    interpretation <- "Classifier Predictions match Causally-Derived ATE estimates. Your Classifier is healthy!"
  } else if (all(dfr$flag > dfr$zmax) | all(dfr$flag < dfr$zmin)) {
    interpretation <- "Classifier Predictions do not match Causally-Derived ATE estimates. Your Classifier is to be considered unreliable. Consider looking into why this might be."
  } else {
    interpretation <- "Classifier Predictions are mixed with respect to Causally-Derived ATE estimates. Use with caution and consider looking into why."
  }
  
  
  if (any(between(dfr$flag, dfr$z1_ATE_LCI[1], dfr$z1_ATE_UCI[1]))) {
    inZ1 <- TRUE
  } else { inZ1 <- FALSE }
  if (any(between(dfr$flag, dfr$z2_ATE_LCI[1], dfr$z2_ATE_UCI[1]))) {
    inZ2 <- TRUE
  } else { inZ2 <- FALSE }
  
  maxflag <- max(dfr$z1_ATE_LCI, dfr$z1_ATE_UCI, dfr$z2_ATE_LCI, dfr$z2_ATE_UCI)
  minflag <- min(dfr$z1_ATE_LCI, dfr$z1_ATE_UCI, dfr$z2_ATE_LCI, dfr$z2_ATE_UCI)
  flagdir <- case_when(maxflag < 0 ~ "a negative",
                       minflag > 0 ~ "a positive",
                       TRUE ~ "no")
  effect_estimation <- case_when(any(abs(dfr$flag) < min(abs(maxflag), abs(minflag))) ~ "underestimating",
                                 any(abs(dfr$flag) > max(abs(maxflag), abs(minflag))) ~ "overestimating",
                                 TRUE ~ "correctly estimating")
  effect_percent <- case_when(effect_estimation == "underestimating" ~ paste0(" by ", round(abs(maxflag) - abs(mean(dfr$flag)), 2)*100, "-",round(abs(minflag) - abs(mean(dfr$flag)), 2)*100, "%"),
                              effect_estimation == "overestimating" ~ paste0(" by ", round(abs(mean(dfr$flag)) - abs(minflag), 2)*100, "-",round(abs(mean(dfr$flag)) - abs(maxflag), 2)*100, "%"),
                              TRUE ~ "")
  effect_fortune <- case_when(inZ1 & inZ2 ~ "Fortunately",
                              TRUE ~ "Unfortunately")
  
  result_text <- paste0("Your classifier is ",
                        effect_estimation, 
                        " the effect that ", 
                        df_vars[1,][[2]], 
                        " is having on ", 
                        df_vars[2,][[2]], 
                        effect_percent, 
                        ". AIR predicts that ", 
                        df_vars[1,][[2]], 
                        " should be having ", 
                        flagdir, 
                        " effect on ", 
                        df_vars[2,][[2]],
                        ". As ", 
                        df_vars[1,][[2]], 
                        " changes, the outcome of ", 
                        df_vars[2,][[2]], 
                        " is ", 
                        case_when(flagdir == "a negative" ~ paste0("between ", round(min(abs(minflag),abs(maxflag)), 2)*100,"-",round(max(abs(minflag),abs(maxflag)), 2)*100,"% less likely to occur. "),
                                  flagdir == "a positive" ~ paste0("between ", round(min(abs(minflag),abs(maxflag)), 2)*100,"-",round(max(abs(minflag),abs(maxflag)), 2)*100,"% more likely to occur. "),
                                  TRUE ~ "unlikely to change. "),
                        effect_fortune,
                        ", your classifier is producing ",
                        case_when(inZ1 & inZ2 ~ "un",
                                  inZ1 | inZ2 ~ "potentially-",
                                  TRUE ~ ""),
                        "biased results, suggesting ",
                        case_when(effect_estimation == "underestimating" ~ "a decreased ",
                                  effect_estimation == "overestimating" ~ "an increased ",
                                  TRUE ~ "an appropriate "),
                        "change in likelihood of ", 
                        df_vars[2,][[2]],
                        " as ",
                        df_vars[1,][[2]],
                        " changes. ",
                        case_when(inZ1 & inZ2 ~ "No bias is detected at this time.",
                                  inZ1 == TRUE & inZ2 == FALSE ~ paste0("Bias is likely being introduced into the training process at variable(s): ", paste0(Zvars$Z[2], collapse = ", "), " (see graph)."),
                                  inZ2 == TRUE & inZ1 == FALSE ~ paste0("Bias is likely being introduced into the training process at variable(s): ", paste0(Zvars$Z[1], collapse = ", "), " (see graph)."),
                                  TRUE ~ paste0("Bias is likely being introduced into the training process at variable(s): ", paste0(Zvars$Z[1], collapse = ", "), " and/or ", paste0(Zvars$Z[2], collapse = ", ")," (see graph)."))
  )
  return(result_text)
}

get_histogram_x <- function(df, xvar, tv_dir, tv_threshold) {
  data <- df[[xvar]]
  
  # Ensure the selected variable is numeric
  # validate(
  #   need(is.numeric(data), "Selected variable must be numeric.")
  # )
  
  # Define treatment condition based on operator and threshold
  if (tv_dir == ">") {
    treated <- data > tv_threshold
    treated_label <- paste0("Treated (>", tv_threshold, ")")
    untreated_label <- paste0("Untreated (≤ ", tv_threshold, ")")
  } else if (tv_dir == "<") {
    treated <- data < tv_threshold
    treated_label <- paste0("Treated (<", tv_threshold, ")")
    untreated_label <- paste0("Untreated (≥ ", tv_threshold, ")")
  } else if (tv_dir == ">=") {
    treated <- data >= tv_threshold
    treated_label <- paste0("Treated (>=", tv_threshold, ")")
    untreated_label <- paste0("Untreated (> ", tv_threshold, ")")
  } else if (tv_dir == "<=") {
    treated <- data < tv_threshold
    treated_label <- paste0("Treated (<=", tv_threshold, ")")
    untreated_label <- paste0("Untreated (> ", tv_threshold, ")")
  } else if (tv_dir == "=") {
    treated <- data == tv_threshold
    treated_label <- paste0("Treated (= ", tv_threshold, ")")
    untreated_label <- "Untreated (≠)"
  }
  
  # Create a dataframe for plotting
  plot_df <- data.frame(
    x = data,
    Treatment = ifelse(treated, "Treated", "Untreated")
  )
  
  # Define colors
  colors <- c("Treated" = "#5D9AFF", "Untreated" = "#EAE1D7")
  
  # Generate the histogram
  ggplot(plot_df, aes(x = x, fill = Treatment, color = Treatment)) +
    geom_rug(sides = "b") +
    geom_histogram(binwidth = (max(data) - min(data)) / 30, color = "black") +#, alpha = 0.7) +
    scale_fill_manual(values = colors, labels = c(untreated_label, treated_label)) +
    scale_color_manual(values = colors, labels = c(untreated_label, treated_label)) +
    geom_vline(xintercept = tv_threshold, color = "gray20", linetype = "dashed", linewidth = 1) +
    labs(
      title = paste("Distribution of ", xvar),
      x = NULL,
      # y = "Count",
      fill = "Treatment Status"
    ) +
    guides(color = "none") +  
    theme_minimal() + 
    theme(text = element_text(color = "#666666", face = "bold"),
          panel.background = element_rect(fill = "transparent", color = NA),
          plot.background  = element_rect(fill = "transparent", color = NA),
          legend.position = "bottom",
          legend.direction = "horizontal") 
}

get_histogram_y <- function(df, yvar, ov_dir, ov_threshold) {
  data <- df[[yvar]]
  
  # Ensure the selected variable is numeric
  # validate(
  #   need(is.numeric(data), "Selected variable must be numeric.")
  # )
  
  # Define treatment condition based on operator and threshold
  if (ov_dir == ">") {
    success <- data > ov_threshold
    success_label <- paste0("Success (>", ov_threshold, ")")
    fail_label <- paste0("Fail (≤ ", ov_threshold, ")")
  } else if (ov_dir == "<") {
    success <- data < ov_threshold
    success_label <- paste0("Success (<", ov_threshold, ")")
    fail_label <- paste0("Fail (≥ ", ov_threshold, ")")
  } else if (ov_dir == ">=") {
    success <- data >= ov_threshold
    success_label <- paste0("Success (>=", ov_threshold, ")")
    fail_label <- paste0("Fail (> ", ov_threshold, ")")
  } else if (ov_dir == "<=") {
    success <- data < ov_threshold
    success_label <- paste0("Success (<=", ov_threshold, ")")
    fail_label <- paste0("Fail (> ", ov_threshold, ")")
  } else if (ov_dir == "=") {
    success <- data == ov_threshold
    success_label <- paste0("Success (= ", ov_threshold, ")")
    fail_label <- "Fail (≠)"
  }
  
  # Create a dataframe for plotting
  plot_df <- data.frame(
    x = data,
    success = ifelse(success, "Success", "Fail")
  )
  
  # Define colors
  colors <- c("Success" = "#5D9AFF", "Fail" = "#EAE1D7")
  # Generate the histogram
  
  ggplot(plot_df, aes(x = x, fill = success, color = success)) +
    geom_rug(sides = "b") +
    geom_histogram(binwidth = (max(data) - min(data)) / 30, color = "black") +#, alpha = 0.7) +
    scale_fill_manual(values = colors, labels = c(fail_label, success_label)) +
    scale_color_manual(values = colors, labels = c(fail_label, success_label)) +
    geom_vline(xintercept = ov_threshold, color = "gray20", linetype = "dashed", linewidth = 1) +
    labs(
      title = paste("Distribution of ", yvar),
      x = NULL,
      # y = "Count",
      fill = "Treatment Status"
    ) +
    guides(color = "none") +  
    theme_minimal() + 
    theme(text = element_text(color = "#666666", face = "bold"),
          panel.background = element_rect(fill = "transparent", color = NA),
          plot.background  = element_rect(fill = "transparent", color = NA),
          legend.position = "bottom",
          legend.direction = "horizontal") 
}

get_updated_graph <- function(dot, graph_update, xvar, yvar, Zvars) { 
  if (graph_update) {
    dot <- change_node_color(dot, xvar, "'#FFC107'")
    dot <- change_node_color(dot, yvar, "'#FFC107'")
    dot <- change_node_color(dot, Zvars[Zvars$grp == "Z1",]$Z, "'#9394A2'")
    dot <- change_node_color(dot, Zvars[Zvars$grp == "Z2",]$Z, "'#D4C7C7'")
  }
  return(dot)
}

get_final_graph <- function(dot, xvar, yvar, Zvars, dfr = results_out_buffer()) {
  # Highlight treatment and outcome nodes
  dot <- change_node_color(dot, xvar, "'#FFC107'")
  dot <- change_node_color(dot, yvar, "'#FFC107'")

  # Extract Z1 and Z2 sets
  Z1 <- Zvars[Zvars$grp == "Z1", ]$Z
  Z2 <- Zvars[Zvars$grp == "Z2", ]$Z

  # Ribbon plot flag logic to color additional nodes
  if (any(dfr$flag > dfr$z1_ATE_UCI & dfr$flag < dfr$z2_ATE_UCI)) {
    dot <- change_node_color(dot, Z2, "'#FFC107'")
  } else if (any(dfr$flag > dfr$z1_ATE_LCI & dfr$flag < dfr$z2_ATE_LCI)) {
    dot <- change_node_color(dot, Z1, "'#FFC107'")
  } else if (any(dfr$flag > dfr$z1_ATE_UCI & dfr$flag > dfr$z2_ATE_UCI)) {
    dot <- change_node_color(dot, Z1, "'#C00000'")
    dot <- change_node_color(dot, Z2, "'#C00000'")
  } else if (any(dfr$flag < dfr$z1_ATE_UCI & dfr$flag < dfr$z2_ATE_UCI)) {
    dot <- change_node_color(dot, Z1, "'#C00000'")
    dot <- change_node_color(dot, Z2, "'#C00000'")
  }

  return(dot)
}

get_ggplot_image_data <- function(plot_obj, width = 8, height = 6, dpi = 300) {
  img <- magick::image_graph(width = width * dpi, height = height * dpi, res = dpi)
  print(plot_obj)
  dev.off()
  img_data <- magick::image_write(img, format = "png", density = dpi)
  return(img_data)
}

graphviz_to_png_data <- function(dot_code) {
  library(DiagrammeR)
  library(rsvg)
  library(xml2)

  svg_str <- DiagrammeRsvg::export_svg(grViz(dot_code))
  svg_obj <- xml2::read_xml(svg_str)

  # Write PNG to raw vector
  raw_conn <- rawConnection(raw(0), open = "wb")
  on.exit(close(raw_conn), add = TRUE)

  rsvg::rsvg_png(svg_obj, file = raw_conn)

  rawConnectionValue(raw_conn)
}
# --- END INLINE: scripts/AIR_functions.R ---

disconnectMessage2()
actionButton("disconnect", "Disconnect the app")

if (!dir.exists(paste0(AIRHome, "/data/"))) {
  dir.create(paste0(AIRHome, "/data/"), recursive = TRUE)
}

if (!dir.exists(paste0(AIRHome, "/input/"))) {
  dir.create(paste0(AIRHome, "/input/"), recursive = TRUE)
}

if (!dir.exists(paste0(AIRHome, "/plots/"))) {
  dir.create(paste0(AIRHome, "/plots/"), recursive = TRUE)
}

if (!dir.exists(paste0(AIRHome, "/logs/"))) {
  dir.create(paste0(AIRHome, "/logs/"), recursive = TRUE)
}

if (Sys.info()["sysname"] == "Linux") {
  # --- BEGIN INLINE: scripts/tetrad_utils.R ---
# ----- Utility Functions -----

# Constants for Java JDK URLs
TETRAD_PATH <- Sys.getenv("TETRAD_PATH")

if (TETRAD_PATH == "") {
  stop("The TETRAD_PATH environment variable is not set. Please set it to the path of the Tetrad jar.")
}

# Function to create the variable list (ArrayList<Node>)
#' Create variable list for Covariance Matrix
#'
#' This function creates an ArrayList of Nodes from the data frame's column names
#' to be used in the Covariance Matrix.
#'
#' @param data The data frame containing the variables.
#' @return A Java List of Nodes.
create_variables <- function(data) {
  #cat("Creating variable list from data...\n")
  
  vars <- .jnew("java/util/ArrayList")
  
  for (name in colnames(data)) {
    #cat("Adding variable:", name, "to the list.\n")
    variable <- .jnew("edu/cmu/tetrad/data/ContinuousVariable", name)
    node <- .jcast(variable, "edu/cmu/tetrad/graph/Node")
    .jcall(vars, "Z", "add", .jcast(node, "java/lang/Object"))
  }
  
  vars <- .jcast(vars, "java/util/List")
  #cat("Variable list creation complete. Number of variables added:", length(vars), "\n")
  
  return(vars)
}

# ----- Initialize Java and Check Version -----
initialize_java <- function() {
  library(rJava)
  
   .jinit()
  .jaddClassPath(TETRAD_PATH)

  java_version <- .jcall("java/lang/System", "S", "getProperty", "java.version")
  print(paste("Java version:", java_version))
}

# ----- Graph Visualization -----
visualize_graph <- function(graph) {
  if (requireNamespace("rstudioapi", quietly = TRUE) && rstudioapi::isAvailable()) {
    b <- TRUE
  } else if (Sys.getenv("RSTUDIO") == "1") {
    b <- TRUE
  } else {
    b <- FALSE
  }
  
  if (b) {
    if (!is.null(graph)) {
      dot <- .jcall("edu/cmu/tetrad/graph/GraphSaveLoadUtils", "Ljava/lang/String;", "graphToDot", graph)
      grViz(dot)
    } else {
      cat("No graph generated. Please check the BOSS execution.\n")
    }
  }
}
  # --- END INLINE: scripts/tetrad_utils.R ---
# --- BEGIN INLINE: scripts/TetradSearch.R ---
# This class translates some select methods from TetradSearch.py in py-tetrad
# for use in R using rJava.
#
# This is a temporary class, as a much better effort at translating these
# methods is underway by another group.

TetradSearch <- setRefClass(
  "TetradSearch",
  
  fields = list(
    data = "data.frame",          # Input dataset
    sample_size = "numeric",      # Sample size
    data_model = "ANY",           # Data Model (Tabular data or Covariance Matrix)
    score = "ANY",                # Score object
    test = "ANY",                 # IndependenceTest object
    mc_test = "ANY",              # IndependenceTest for the Markov Checker
    mc_ind_results = "ANY",       # Markov Checker independence test results
    knowledge = "ANY",            # Background knowledge object
    graph = "ANY",                # Resulting graph
    search = "ANY",               # Search object
    params = "ANY"                # Parameters object
  ),
  
  methods = list(
    
    # Initialize the TetradSearch object
    #
    # @param data A data frame containing the dataset to be analyzed.
    # @return A TetradSearch object.
    initialize = function(data) {
      cat("Initializing TetradSearch object...\n")
      
      if (!is.data.frame(data)) {
        stop("Data must be a data.frame")
      }
      
      .self$data <- data
      .self$sample_size <- nrow(data)
      cat("Data frame dimensions:", dim(data), "\n")
      cat("Sample size set to:", .self$sample_size, "\n")
      
      .self$data_model <- .self$data_frame_to_tetrad_dataset(data)
      .self$data_model <- .jcast(.self$data_model, "edu.cmu.tetrad.data.DataModel")
      
      cat("Tetrad DataSet created.\n")
      
      .self$params <- .jnew("edu.cmu.tetrad.util.Parameters")
      
      .self$knowledge <- .jnew("edu/cmu/tetrad/data/Knowledge")
      cat("Knowledge instance created.\n")
      cat("TetradSearch object initialized successfully.\n")
    },
    
    # Make sure the score object is initialized
    .check_score = function() {
      if (is.null(.self$score)) {
        stop("Error: The 'score' field has not been initialized yet. Please \
                 set a score before running the algorithm.")
      }
    },
    
    .setParam = function(key, value) {
      .jcall(.self$params, "V", "set", key, .jcast(.jnew("java/lang/Boolean", value), "java/lang/Object"))
    },
    
    .setParamInt = function(key, value) {
      .jcall(.self$params, "V", "set", key, .jcast(.jnew("java/lang/Integer", as.integer(value)), "java/lang/Object"))
    },
    
    .set_knowledge = function() {
      .jcall(.self$search, "V", "setKnowledge", .jcast(.self$knowledge, "edu.cmu.tetrad.data.Knowledge"))
    },
    
    # Run the search algorithm, for the typical case
    .run_search = function() {
      .self$.set_knowledge()
      .self$graph <- .jcast(.self$search$search(), "edu.cmu.tetrad.graph.Graph")
    },
    
    # Make sure the test object is initialized
    .check_test = function() {
      if (is.null(.self$test)) {
        stop("Error: The 'test' field has not been initialized yet. Please \
                 set a test before running the algorithm.")
      }
    },
    
    # Add a variable to a specific tier in the knowledge
    #
    # @param tier The tier to which the variable should be added.
    # @param var_name The name of the variable to add.
    add_to_tier = function(tier, var_name) {
      cat("Adding variable", var_name, "to tier", tier, "...\n")
      tryCatch({
        tier <- as.integer(tier)
        var_name <- as.character(var_name)
        .jcall(.self$knowledge, "V", "addToTier", tier, var_name)
        cat("Variable", var_name, "added to tier", tier, ".\n")
      }, error = function(e) {
        cat("Error adding variable to tier:", e$message, "\n")
      })
    },
    
    # Set the verbose flag
    #
    # @param verbose TRUE or FALSE
    set_verbose = function(verbose) {
      .self$.setParam("verbose", verbose)
    },
    
    # Set the score to the SEM BIC.
    #
    # @param penalty_discount The penalty discount to use in the SemBicScore calculation.
    use_sem_bic = function(penalty_discount = 2) {
      .self$.setParamDouble("penaltyDiscount", penalty_discount)
      .self$score <- .jnew("edu.cmu.tetrad.algcomparison.score.SemBicScore")
      .self$score <- .jcast(.self$score, "edu.cmu.tetrad.algcomparison.score.ScoreWrapper")
      cat("SemBicScore object created with penalty discount set.\n")
    },
    
    # Set the test to Fisher Z
    #
    # @param alpha The significance cutoff.
    use_fisher_z = function(alpha = 0.01, use_for_mc = FALSE) {
      .self$.setParamDouble("alpha", alpha)
      
      if (use_for_mc) {
        .self$mc_test <- .jnew("edu.cmu.tetrad.algcomparison.independence.FisherZ")
        .self$mc_test <- .jcast(.self$mc_test, "edu.cmu.tetrad.algcomparison.independence.IndependenceWrapper")
      } else {
        .self$test <- .jnew("edu.cmu.tetrad.algcomparison.independence.FisherZ")
        .self$test <- .jcast(.self$test, "edu.cmu.tetrad.algcomparison.independence.IndependenceWrapper")
      }
      
      cat("Fisher Z object created with alpha set.\n")
    },
    
    # --- Internal parameter helpers ---
    
    .setParamDouble = function(key, value) {
      .jcall(.self$params, "V", "set", key, .jcast(.jnew("java/lang/Double", as.double(value)), "java/lang/Object"))
    },
    
    # Run the BOSS algorithm
    #
    # @param num_starts The number of random restarts to do; the model with the best BIC score overall is returned.
    # @param use_bes TRUE if the algorithm should finish up with a call to BES (Backward Equivalence Search from
    #   the FGES algorithm) to guarantee correctness under Faithfulness.
    # @param time_lag Default 0; if > 1, a time lag model of this order is constructed.
    # @param use_data_order TRUE if the original data order should be used for the initial permutation. If
    #   num_starts > 1, random permuatations are used for subsequent restarts.
    # @param output_cpdag TRUE if a CPDAG should be output, FALSE if a DAG should be output.
    # @return The estimated graph.
    run_boss = function(num_starts = 1, use_bes = FALSE, time_lag = 0, use_data_order = TRUE, output_cpdag = TRUE) {
      cat("Running BOSS algorithm...\n")
      
      .self$.setParam("useBes", use_bes)
      .self$.setParamInt("numStarts", num_starts)
      .self$.setParamInt("timeLag", time_lag)
      .self$.setParam("useDataOrder", use_data_order)
      .self$.setParam("outputCpdag", output_cpdag)
      
      dataModel <- .jcast(.self$data_model, "edu.cmu.tetrad.data.DataModel")
      
      boss <- .jnew("edu.cmu.tetrad.algcomparison.algorithm.oracle.cpdag.Boss", .self$score)
      .jcall(boss, "V", "setKnowledge", .self$knowledge)
      
      graph <<- .jcall(boss, "Ledu/cmu/tetrad/graph/Graph;", "search", dataModel, .self$params)
      .self$graph <- graph
      
      cat("BOSS search completed.\n")
    },

    get_java = function() {
      return(.self$graph)
    },
    
    # This method prints the structure of the graph estimated by the most recent algorithm call.
    print_graph = function() {
      cat("Attempting to print the graph...\n")
      if (is.null(.self$graph)) {
        cat("No graph generated yet. Please run an algorithm first.\n")
      } else {
        cat("Graph structure:\n", .self$graph$toString(), "\n")
      }
      invisible(.self$graph)
    },
    
    # An adjustment set for a pair of nodes <source, target> for a CPDAG is a set of nodes that blocks
    # all paths from the source to the target that cannot contribute to a calculation for the total effect
    # of the source on the target in any DAG in a CPDAG while not blocking any path from the source to the target
    # that could be causal. In typical causal graphs, multiple adjustment sets may exist for a given pair of
    # nodes. This method returns up to maxNumSets adjustment sets for the pair of nodes <source, target>
    # fitting a certain description.
    #
    # The description is as follows. We look for adjustment sets of variables that are close to either the
    # source or the target (or either) in the graph. We take all possibly causal paths from the source to the
    # target into account but only consider other paths up to a certain specified length. (This maximum length
    # can be unlimited for small graphs.)
    #
    # Within this description, we list adjustment sets in order or increasing size. Hopefully, these parameters
    # along with the size ordering can help to give guidance for the user to choose the best adjustment set for
    # their purposes when multiple adjustment sets are possible.
    #
    # @param source                  The source node whose sets will be used for adjustment.
    # @param target                  The target node whose sets will be adjusted to match the source node.
    # @param maxNumSets              The maximum number of sets to be adjusted. If this value is less than or equal to
    #                                0, all sets in the target node will be adjusted to match the source node.
    # @param maxDistanceFromEndpoint The maximum distance from the endpoint of the trek to consider for adjustment.
    # @param nearWhichEndpoint       The endpoint(s) to consider for adjustment; 1 = near the source, 2 = near the
    #                                target, 3 = near either.
    # @param maxPathLength           The maximum length of the path to consider for backdoor paths. If a value of -1 is
    #                                given, all paths will be considered.
    # @return A list of adjustment sets for the pair of nodes &lt;source, target&gt;. Return an smpty
    # list if source == target or there is no amenable path from source to target.
    get_adjustment_sets = function(graph, source, target, max_num_sets = 10, max_distance_from_point = 5,
                                   near_which_endpoint = 1, max_path_length = 20) {
      cat("Getting adjustment sets for:", source, "->", target, "\n")
      
      # Look up Node objects by name
      source_node <- .jcall(graph, "Ledu/cmu/tetrad/graph/Node;", "getNode", source)
      target_node <- .jcall(graph, "Ledu/cmu/tetrad/graph/Node;", "getNode", target)
      
      if (is.jnull(source_node)) stop(paste("Source node", source, "not found in the graph."))
      if (is.jnull(target_node)) stop(paste("Target node", target, "not found in the graph."))
      
      # Get Paths object from Graph
      paths <- .jcall(graph, "Ledu/cmu/tetrad/graph/Paths;", "paths")
      
      # Java List<Set<Node>>
      sets_list <- .jcall(paths,
                          "Ljava/util/List;",
                          "adjustmentSets",
                          source_node,
                          target_node,
                          as.integer(max_num_sets),
                          as.integer(max_distance_from_point),
                          as.integer(near_which_endpoint),
                          as.integer(max_path_length))
      
      
      size <- .jcall(sets_list, "I", "size")
      cat("Number of adjustment sets:", size, "\n")
      
      # Convert Java List<Set<Node>> to R list of character vectors
      size <- .jcall(sets_list, "I", "size")
      result <- vector("list", size)
      
      for (i in seq_len(size)) {
        jset <- .jcall(sets_list, "Ljava/lang/Object;", "get", as.integer(i - 1))
        jarray <- .jcall(jset, "[Ljava/lang/Object;", "toArray")
        result[[i]] <- sapply(jarray, function(n) .jcall(n, "S", "getName"))
      }
      
      return(result)
    },
    
    print_adjustment_sets = function(adjustment_sets) {
      if (length(adjustment_sets) == 0) {
        cat("No adjustment sets found.\n")
        return()
      }
      
      for (i in seq_along(adjustment_sets)) {
        set <- adjustment_sets[[i]]
        cat(sprintf("Adjustment set %d: ", i))
        if (length(set) == 0) {
          cat("(empty set)\n")
        } else {
          cat(paste(set, collapse = ", "), "\n")
        }
      }
    },
    
    # Performs a Markov check on a graph with respect to the supplied dataset and returns statistics
    # showing performance on that check.
    #
    # @param graph The graph to perform the Markov check on. This may be a DAG, CPDAG, MAG or PAG.
    # @param percent_resample Tests are done using random subsamples of the data per test, if this is
    #   less than 1, or all of the data, if it is equal to 1.
    # @param condition_set_type The type of conditioning set to use for the Markov check, one of:
    #   GLOBAL_MARKOV, LOCAL_MARKOV, PARENTS_AND_NEIGHBORS, MARKOV_BLANKET, RECURSIVE_MSEP, NONCOLLIDERS_ONLY,
    #   ORDERED_LOCAL_MARKOV, or ORDERED_LOCAL_MARKOV_MAG
    # @param find_smallest_subset Whether to find the smallest subset for a given set that yields independence.
    # @param parallelized TRUE if conditional independencies should be checked in parallel.
    # @effective_sample_size The effective sample size to use for calculations, or -1 if the actual sample size.
    # @return Marov checker statistics as a named list.
    markov_check = function(graph, percent_resample = 1, condition_set_type = "ORDERED_LOCAL_MARKOV",
                            find_smallest_subset = FALSE, parallelized = TRUE, effective_sample_size = -1) {
      cat("Running Markov check...\n")
      
      if (is.null(.self$mc_test)) {
        stop("A test for the Markov Checker has not been set. Please call a `use_*` method with `use_for_mc = TRUE`.")
      }
      
      condition_set_type_ <- .jfield("edu.cmu.tetrad.search.ConditioningSetType",
                                     name = condition_set_type,
                                     sig = "Ledu/cmu/tetrad/search/ConditioningSetType;")
      
      dataModel <- .jcast(.self$data_model, "edu.cmu.tetrad.data.DataModel")
      
      test_ <- .jcall(.self$mc_test, "Ledu/cmu/tetrad/search/IndependenceTest;",
                      "getTest", dataModel, .self$params)
      
      mc <- .jnew("edu.cmu.tetrad.search.MarkovCheck", graph, test_, condition_set_type_)
      
      # Configure it
      .jcall(mc, "V", "setPercentResample", as.double(percent_resample))
      .jcall(mc, "V", "setFindSmallestSubset", find_smallest_subset)
      .jcall(mc, "V", "setParallelized", parallelized)
      
      # Generate results
      .jcall(mc, "V", "generateAllResults")
      .self$mc_ind_results <- .jcall(mc, "Ljava/util/List;", "getResults", TRUE)
      
      # Set sample size if specified
      ### temporary fix ((mdk)) ??
      ### originally: if (sample_size != -1) {
      if(effective_sample_size != -1) {
        ### temporary fix ((mdk)) ??   
        ### originally: .jcall(mc, "V", "setSampleSize", as.integer(sample_size))
        .jcall(mc, "V", "setSampleSize", as.integer(effective_sample_size))
      }
      
      # Extract statistics
      ad_ind <- .jcall(mc, "D", "getAndersonDarlingP", TRUE)
      ad_dep <- .jcall(mc, "D", "getAndersonDarlingP", FALSE)
      ks_ind <- .jcall(mc, "D", "getKsPValue", TRUE)
      ks_dep <- .jcall(mc, "D", "getKsPValue", FALSE)
      bin_indep <- .jcall(mc, "D", "getBinomialPValue", TRUE)
      bin_dep <- .jcall(mc, "D", "getBinomialPValue", FALSE)
      frac_dep_ind <- .jcall(mc, "D", "getFractionDependent", TRUE)
      frac_dep_dep <- .jcall(mc, "D", "getFractionDependent", FALSE)
      num_tests_ind <- .jcall(mc, "I", "getNumTests", TRUE)
      num_tests_dep <- .jcall(mc, "I", "getNumTests", FALSE)
      
      # Return as a named list
      return(list(
        ad_ind = ad_ind,
        ad_dep = ad_dep,
        ks_ind = ks_ind,
        ks_dep = ks_dep,
        bin_indep = bin_indep,
        bin_dep = bin_dep,
        frac_dep_ind = frac_dep_ind,
        frac_dep_dep = frac_dep_dep,
        num_tests_ind = num_tests_ind,
        num_tests_dep = num_tests_dep,
        mc = mc
      ))
    },
    
    # Converts the given R data frame to a (possibly mixed) Tetrad DataSet.
    #
    # @param df The R data frame to translate. Continuous columns should be of type 'numeric' and the
    #   discrete columns of type 'integer'.
    data_frame_to_tetrad_dataset = function(df) {
      stopifnot(require(rJava))
      
      nrows <- nrow(df)
      ncols <- ncol(df)
      
      # Create Java ArrayList<Node>
      var_list <- .jnew("java/util/ArrayList")
      
      # Prepare empty double[][] and int[][] (as Java arrays)
      cont_data <- vector("list", ncols)
      disc_data <- vector("list", ncols)
      
      for (j in seq_len(ncols)) {
        name <- colnames(df)[j]
        col <- df[[j]]
        
        if (is.numeric(col)) {
          variable <- .jnew("edu/cmu/tetrad/data/ContinuousVariable", name)
          node <- .jcast(variable, "edu/cmu/tetrad/graph/Node")
          .jcall(var_list, "Z", "add", .jcast(node, "java/lang/Object"))
          cont_data[[j]] <- .jarray(as.numeric(col), dispatch = TRUE)
          disc_data[[j]] <- .jnull("[I")  # null int[] for discrete
        } else if (is.integer(col) || is.factor(col)) {
          num_categories <- length(unique(na.omit(col)))
          variable <- .jnew("edu/cmu/tetrad/data/DiscreteVariable", name, as.integer(num_categories))
          node <- .jcast(variable, "edu/cmu/tetrad/graph/Node")
          .jcall(var_list, "Z", "add", .jcast(node, "java/lang/Object"))
          cont_data[[j]] <- .jnull("[D")  # null double[] for continuous
          disc_data[[j]] <- .jarray(as.integer(col), dispatch = TRUE)
        } else {
          stop(paste("Unsupported column type:", name))
        }
      }
      
      # Convert R lists of arrays to Java double[][] and int[][]
      j_cont_data <- .jarray(cont_data, dispatch = TRUE)
      j_disc_data <- .jarray(disc_data, dispatch = TRUE)
      
      # Call static Java helper method
      ds <- .jcall("edu.cmu.tetrad.util.DataSetHelper",
                   "Ledu/cmu/tetrad/data/DataSet;",
                   "fromR",
                   .jcast(var_list, "java.util.List"),
                   as.integer(nrows),
                   .jcast(j_cont_data, "[[D"),
                   .jcast(j_disc_data, "[[I"))
      
      return(ds)
    }
  )
)
# --- END INLINE: scripts/TetradSearch.R ---
  
  if (!dir.exists(paste0(AIRHome, "/data/"))) {
    dir.create(paste0(AIRHome, "/data/"), recursive = TRUE)
  }
  
  if (!dir.exists(paste0(AIRHome, "/input/"))) {
    dir.create(paste0(AIRHome, "/input/"), recursive = TRUE)
  }
  
  if (!dir.exists(paste0(AIRHome, "/plots/"))) {
    dir.create(paste0(AIRHome, "/plots/"), recursive = TRUE)
  }
  
  if (!dir.exists(paste0(AIRHome, "/logs/"))) {
    dir.create(paste0(AIRHome, "/logs/"), recursive = TRUE)
  }
  # Setup Java and Tetrad
  ## if running on local machine, uncomment out the next line
  # setup_tetrad_environment()
  
  Sys.unsetenv("_JAVA_OPTIONS")
  .jinit()
  # .jinit(parameters = "-verbose:class")
  .jaddClassPath(TETRAD_PATH)
  
  java_version <- .jcall("java/lang/System", "S", "getProperty", "java.version")
  # print(paste("Java version:", java_version))
} 

## create a log file, then remove any logs over 30 days old
log_file <- paste0("logs/", format(Sys.time(), "%Y-%m-%d_%H-%M-%S"), "_error_log.txt")
cat("Begin Log\n", file = log_file, append = FALSE)
# Define the log directory and calculate the time threshold (7 days ago)
log_dir <- "logs"
time_threshold <- Sys.time() - 30 * 24 * 60 * 60  # 30 days in seconds

# List all log files in the directory
log_files <- list.files(log_dir, full.names = TRUE)

# Get file information for each file
files_info <- file.info(log_files)

# Identify files where the modification time is older than the threshold
old_files <- rownames(files_info)[files_info$mtime < time_threshold]

# Remove the old log files
if (length(old_files) > 0) {
  file.remove(old_files)
}
```

# Docs

```{r info-tab, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# Read the file and output its content as-is
knitr::opts_knit$set(root.dir = AIRHome)
knitr::opts_chunk$set(progress = FALSE)
options(knitr.progress = FALSE)

cat(knitr::knit_child("info.qmd", quiet = TRUE, envir = knitr::knit_global()))
```

# Analysis of Bias

##  {.sidebar}

```{r sidebar}
h4("Step 1: Upload your data")
fileInput("file1", "", accept = ".csv")
br()
uiOutput('step2')
uiOutput('ui_file2')
br()
uiOutput('ui_buildButton')
br()
uiOutput('step3')
uiOutput('xvar')
uiOutput('ui_threshold_x')
br()
uiOutput('yvar')
uiOutput('ui_threshold_y')
br()
uiOutput('ui_updateButton')
uiOutput('step4')
uiOutput('ui_model_exist')
uiOutput('ui_model_upload')
uiOutput('ui_ate_upload')
uiOutput('ui_goButton')
uiOutput('ui_dl_btn')

```

## Column {width = "40%"}

```{r graph-pane}
# grVizOutput('blankGraph')
uiOutput('ui_graph_pane')
```

## Column {width = "60%"}

```{r right-of-graph}
# plotOutput('histogram_x', height = "50%")
uiOutput('second_column_content')
# uiOutput('ui_top_right_pane')
```

```{r backend-compute}
#| context: server


### variable declarations --------------------------------------------------------------
# --- Reactive value containers --- #
calc_complete <- reactiveVal(FALSE)
graph_complete <- reactiveVal(FALSE)
graph_update <- reactiveVal(FALSE)
file_check <- reactiveVal(FALSE)


### input reactives -------------------------------------------------------------------
## ------------------------------------------------------------
## Read either .rds or .rda / .RData and return the first object
## ------------------------------------------------------------
model_in <- reactive({
  req(input$model_in)

  ## use the original filename to preserve the extension
  ext <- tolower(tools::file_ext(input$model_in$name))

  cat(sprintf("%s - Upload received: %s → %s  (ext = %s)\n",
              Sys.time(),
              input$model_in$name,
              input$model_in$datapath,
              ext),
      file = log_file, append = TRUE)
  tryCatch({

    mdl <- switch(
      ext,

      ## ----------------------------------------------------------------
      ## 1)  readRDS – single-object files
      ## ----------------------------------------------------------------
      "rds" = base::readRDS(input$model_in$datapath),

      ## ----------------------------------------------------------------
      ## 2)  load() – standard .rda / .RData
      ##     – if that fails we *try* readRDS as a fall-back
      ## ----------------------------------------------------------------
      "rda" = , "rdata" = {
        env <- new.env()
        obj_names <- tryCatch(
          base::load(input$model_in$datapath, envir = env),
          error = function(e) {
            message("load() failed, trying readRDS() …")
            NULL
          }
        )

        if (is.null(obj_names)) {
          ## ‘.rda’ file was actually an .rds – give it one more shot
          base::readRDS(input$model_in$datapath)

        } else if (length(obj_names) == 0) {
          stop("No objects found inside the .rda/.RData file")

        } else {
          env[[obj_names[1]]]
        }
      },

      ## ----------------------------------------------------------------
      ## 3)  anything else → hard-fail
      ## ----------------------------------------------------------------
      stop(sprintf("Unsupported model file extension: %s", ext))
    )

    ## ------- basic sanity-check: does it respond to predict() ? ----------
    validate(
      need(is.function(tryCatch(predict(mdl, head(mtcars)), error = function(e) NULL)),
           "Loaded object does not appear to have a usable predict() method.")
    )

    cat(sprintf("%s - Model loaded (%s): %s\n", Sys.time(), ext, input$model_in$name), file = log_file, append = TRUE)
    mdl
  }, error = function(e) {
    cat(sprintf("%s - Error reading model: %s\n", Sys.time(), conditionMessage(e)), file = log_file, append = TRUE)
    NULL
  })
})

df <- reactive({
  tryCatch({
    df <- read_csv(input$file1$datapath, col_types = cols(.default = col_number()))
    cat(sprintf("%s - Read in data file: %s\n", Sys.time(), input$file1$datapath), file = log_file, append = TRUE)
    continuous_columns <- c(1, ncol(df))
    df[, continuous_columns] <- apply(df[, continuous_columns], 2, as.numeric)
    df
  }, error = function(e) {
    cat(sprintf("%s - Error reading data file: %s\n", Sys.time(), conditionMessage(e)), file = log_file, append = TRUE)
    NULL
  })
})

knowledge <- reactive({
  req(input$file2)
  tryCatch({
    path2 <- input$file2$datapath
    cat(sprintf("%s - Read in knowledge file: %s\n", Sys.time(), path2), file = log_file, append = TRUE)

    ext <- tolower(tools::file_ext(path2))
    knowledge_in <- switch(ext,
      csv = read_csv(path2, col_names = TRUE),
      xlsx = readxl::read_excel(path2, col_names = TRUE),
      {
        sendSweetAlert(...); return(NULL)
      }
    )

    fixed <- fix_knowledge(knowledge_in)

    if (is.character(fixed)) {
      sendSweetAlert(...); return(NULL)
    }

    file_check(TRUE)
    fixed
  }, error = function(e) {
    cat(sprintf("%s - Error in knowledge(): %s\n", Sys.time(), conditionMessage(e)), file = log_file, append = TRUE)
    sendSweetAlert(
      session = shiny::getDefaultReactiveDomain(),
      title = "Upload Failed",
      text = "An error occurred while reading the knowledge file.",
      type = "error",
      btn_labels = "Continue"
    )
    NULL
  })
})

### event observations ----------------------------------------------------------------
options(shiny.error = function() {
  tb <- traceback()  # Capture the call stack
  showModal(modalDialog(
    title = "An error occurred",
    paste0("AIR Tool crashed. See log file, ", log_file, ", for details"),
    easyClose = TRUE
  ))
})

# Disconnect observer
observeEvent(input$disconnect, {
  tryCatch({
    session$close()
  }, error = function(e) {
    cat(sprintf("Error in disconnect observer: %s\n", conditionMessage(e)), file = log_file, append = TRUE)
  })
})

# File input triggers reactive evaluation
observeEvent(input$file2, {
  tryCatch({
    result <- knowledge()
    # Optionally log result
    # cat(str(result), file = log_file, append = TRUE)
  }, error = function(e) {
    cat(sprintf("Error in file2 observer: %s\n", conditionMessage(e)), file = log_file, append = TRUE)
  })
})

# Causal graph build button
observeEvent(input$buildButton, {
  tryCatch({

    if (Sys.info()["sysname"] != "Linux") {
      stop("Non-Linux execution path no longer supported: remove or re-implement without file I/O")
    }

    graphlist <- AIR_getGraph(df(), knowledge(), graphtxt_buffer = graphtxt_buffer)

    graphtxt_raw <- .jcall(graphlist[[1]], "Ljava/lang/String;", "toString")

    if (is.null(graphtxt_raw) || !nzchar(graphtxt_raw)) {
      stop("Tetrad returned an empty graph string")
    }

    graphtxt <- tryCatch(
      gsub("(?s)Graph Attributes:.*", "", graphtxt_raw, perl = TRUE),
      error = function(e) {
        warning(sprintf("Regex error on graph string: %s", conditionMessage(e)))
        graphtxt_raw  # fallback to raw if regex fails
      }
    )

    rv$ts <- graphlist[[2]]
    rv$MC_passing_cpdag_already_found <- graphlist[[3]]
    rv$best_cpdag_seen_so_far <- graphlist[[4]]

    dot_raw <- .jcall("edu/cmu/tetrad/graph/GraphSaveLoadUtils", "Ljava/lang/String;", "graphToDot", graphlist[[1]])

    rv$dot <- gsub("(?m)^\\s*\\n", "", dot_raw %||% "", perl = TRUE)

    graph_complete(TRUE)

    output$blankGraph <- renderGrViz({
      req(graph_complete(), rv$dot)
      cat("Graph Built\n", file = log_file, append = TRUE)
      grViz(rv$dot)
    })

    sendSweetAlert(
      session = shiny::getDefaultReactiveDomain(),
      title = "Your Causal Graph is Ready",
      text = NULL,
      type = "success",
      btn_labels = "Continue",
      btn_colors = "#3085d6",
      closeOnClickOutside = TRUE,
      showCloseButton = FALSE
    )

  },
  error = function(e) {
    err_summary <- tryCatch({
      paste0("Error in buildButton observer: ", conditionMessage(e), "\n")
    }, error = function(inner) {
      paste0("Unknown error in buildButton observer: ", capture.output(str(e)), collapse = "\n")
    })
    cat(err_summary, file = log_file, append = TRUE)
  })
})

observeEvent(input$updateButton, {
  tryCatch({

    assign("df_vars",
           data.frame("var" = c("TV", "OV"),
                      "val" = c(input$xvar, input$yvar),
                      stringsAsFactors = FALSE),
           envir = .GlobalEnv)

    if (Sys.info()["sysname"] != "Linux") {
      stop("Non-Linux platforms are not supported without in-memory graph. File fallback removed.")
    }

    adj_list <- AIR_getAdjSets(
      rv$ts,
      input$xvar,
      input$yvar,
      rv$MC_passing_cpdag_already_found,
      rv$best_cpdag_seen_so_far
    )

    Z1 <- strsplit(gsub("[{}']", "", toString(adj_list[[1]]), perl = TRUE), ", ")[[1]]
    Z2 <- strsplit(gsub("[{}']", "", toString(adj_list[[2]]), perl = TRUE), ", ")[[1]]

    Zvars_loc <- rbind(
      data.frame(name = input$xvar, grp = "Z1", Z = Z1, stringsAsFactors = FALSE),
      data.frame(name = input$xvar, grp = "Z2", Z = Z2, stringsAsFactors = FALSE)
    )
    assign("Zvars", Zvars_loc, envir = .GlobalEnv)

    cat("Graph Updated\n", file = log_file, append = TRUE)

    if (graph_update()) {
      sendSweetAlert(
        session = shiny::getDefaultReactiveDomain(),
        title = "Causal Graph Updated",
        text = NULL,
        type = "success",
        btn_labels = "Continue",
        btn_colors = "#3085d6",
        closeOnClickOutside = TRUE,
        showCloseButton = FALSE
      )
    }

    graph_update(TRUE)

    output$blankGraph <- renderGrViz({
      req(graph_complete())
      dot <- .jcall("edu/cmu/tetrad/graph/GraphSaveLoadUtils", "Ljava/lang/String;", "graphToDot", rv$best_cpdag_seen_so_far)
      if (graph_update()) {
        dot <- change_node_color(dot, input$xvar, "'#FFC107'")
        dot <- change_node_color(dot, input$yvar, "'#FFC107'")
        dot <- change_node_color(dot, Zvars[Zvars$grp == "Z1", ]$Z, "'#9394A2'")
        dot <- change_node_color(dot, Zvars[Zvars$grp == "Z2", ]$Z, "'#D4C7C7'")
      }
      grViz(dot)
    })

    cat(sprintf("%s - Selected X variable: %s\n", Sys.time(), input$xvar), file = log_file, append = TRUE)
    cat(sprintf("%s - Selected Y variable: %s\n", Sys.time(), input$yvar), file = log_file, append = TRUE)
    cat(sprintf("%s - Selected X threshold: %s %s\n", Sys.time(), input$tv_dir, input$tv_threshold), file = log_file, append = TRUE)
    cat(sprintf("%s - Selected Y threshold: %s %s\n", Sys.time(), input$ov_dir, input$ov_threshold), file = log_file, append = TRUE)

  }, error = function(e) {
    cat(sprintf("Error in updateButton observer: %s\n", conditionMessage(e)), file = log_file, append = TRUE)
  })
})
  
observeEvent(input$goButton, {
  tryCatch({
    print(ls(pattern = "df", envir = .GlobalEnv))

    cat(paste0(Sys.time(), " - Selected model_exist: ", input$model_exist, "\n"), file = log_file, append = TRUE)
    cat(paste0(Sys.time(), " - Selected ate_in: ", input$ate_in, "\n"), file = log_file, append = TRUE)

    withProgress(message = 'Building Causal Graph', style = "notification", value = 0.1, {
      incProgress(0.1, message = "Calculating Adjustment Sets", detail = "Creating Compute Environment")

      datafile_buffer(df())
      rv$model_yn <- input$model_exist
      rv$model_ate <- input$ate_in

      results_out_buffer(data.frame(
        Row = numeric(),
        Treatment = character(),
        Group = character(),
        Mean = numeric(),
        LCI = numeric(),
        UCI = numeric(),
        stringsAsFactors = FALSE
      ))

      cat("Calculating Adjustment Sets\n", file = log_file, append = TRUE)

      tryCatch({
        combos <- expand.grid(i = unique(Zvars$name), j = unique(Zvars$grp), stringsAsFactors = FALSE)
        tv_dir_val <- input$tv_dir
        tv_threshold_val <- input$tv_threshold
        ov_dir_val <- input$ov_dir
        ov_threshold_val <- input$ov_threshold

        n_cores <- min(nrow(combos), parallel::detectCores() - 1L)
        cluster <- makeCluster(n_cores)
        on.exit(stopCluster(cluster), add = TRUE)

        clusterEvalQ(cluster, {
          library(tmle3)
          library(sl3)
        })

        ## Only truly-static objects go through the automatic
        ## foreach `.export` mechanism
        vars_to_export <- c(
          "Zvars",
          "runSuperLearner",
          "log_file"
        )

        ## while we *explicitly* push the big lookup table once
        ## so every worker sees it
        clusterExport(cluster, "df_vars", envir = environment())

        registerDoParallel(cluster)

        incProgress(0.1, detail = "Calculating ATE for multiple Adjustment Sets")

        # ---- take immutable copies while we're still in the main thread ----
        df_snapshot     <- isolate(datafile_buffer())   # <-- plain data.frame now
        graph_snapshot  <- isolate(rv$dot)              # whatever rv$dot is

        # pass the snapshots to the workers; nothing reactive inside anymore
        results_list <- foreach(k = 1:nrow(combos),
                .packages = c("AIPW", "dplyr", "e1071", "earth", "ggplot2",
                              "hal9001", "nnet", "randomForest", "readr",
                              "scales", "sl3", "tidyr", "tmle3", "xgboost",
                              "foreach", "doParallel", "shiny"),
                .export   = vars_to_export) %dopar% {
          i <- combos[k, "i"]
          j <- combos[k, "j"]
          settings <- data.frame(
            doc_title = paste0(i, "-", j),
            nfold = 20,
            Z_level = j,
            varName = i,
            confounders = paste0(Zvars[Zvars$name == i & Zvars$grp == j, ]$Z, collapse = " ")
          )

          res <- tryCatch(
            runSuperLearner(settings,
                            graph_snapshot,   # was  rv$dot
                            df_snapshot,      # <-- new arg (next step)
                            tv_dir_val, tv_threshold_val,
                            ov_dir_val, ov_threshold_val,
                            log_file),
            error = function(e) {
              cat(sprintf("SuperLearner: Error i=%s, j=%s: %s",
                          i, j, conditionMessage(e)),
                  file = log_file, sep = "\n", append = TRUE)
              NULL
            }
          )
          res
        }

        ## Main thread again.
        for (res in results_list) {
          if (!is.null(res)) {
            results_buffer(
              dplyr::bind_rows(results_buffer(), res$tmle)
            )
            superlearner_output_buffer(
              append(superlearner_output_buffer(), res$diag)
            )
          }
        }

        # -----------------------------------------------------------------
        # Build the final combined data-frame so the UI can see it
        # -----------------------------------------------------------------
        tryCatch({
          # “settings_stub” supplies the few fields processResults() still reads
          settings_stub <- data.frame(
            doc_title  = "Combined",
            nfold      = 20,
            Z_level    = "Z?",
            varName    = input$xvar,
            confounders = ""
          )
        
          processResults(settings_stub,
                         AIRHome       = graph_snapshot,
                         tv_dir        = tv_dir_val,
                         tv_threshold  = tv_threshold_val,
                         ov_dir        = ov_dir_val,
                         ov_threshold  = ov_threshold_val,
                         model_in      = if (identical(input$model_exist, "Yes"))
                                            model_in() else NULL,
                         model_yn      = input$model_exist,
                         model_ate     = input$ate_in,
                         log_file      = log_file)
        }, error = function(e) {
          cat(sprintf("processResults() failed: %s", conditionMessage(e)),
              file = log_file, append = TRUE)
        })

        incProgress(0.2, message = "Processing Results")
        cat("Successfully closed parallel cluster\n", file = log_file, append = TRUE)

      }, error = function(e) {
        cat(sprintf("Fatal error during ATE estimation: %s", conditionMessage(e)), file = log_file, append = TRUE)
      })

      calc_complete(TRUE)

      output$blankGraph <- renderGrViz({
        dot <- get_final_graph(rv$dot, input$xvar, input$yvar, Zvars)
        grViz(dot)
      })

      sendSweetAlert(
        session = shiny::getDefaultReactiveDomain(),
        title = "Causal Estimates Successfully Calculated",
        text = "",
        type = "success",
        btn_labels = "Continue",
        btn_colors = "#3085d6",
        closeOnClickOutside = TRUE,
        showCloseButton = FALSE
      )
    })

  }, error = function(e) {
    cat(paste0(Sys.time(), " - Top-level observeEvent(goButton) failure: ", conditionMessage(e), "\n"),
        file = log_file, append = TRUE)
  })
})

### output definitions ------------------------------------------------------------------

output$ui_graph_pane = renderUI({
  grVizOutput('blankGraph')
})

output$second_column_content <- renderUI({
    if (calc_complete()) {
      tags$div(
        style = "display: flex; flex-direction: column; height: 100%;",
        tags$div(
          style = "flex: 0 0 25%;",
          uiOutput("ui_ribbon_plot")
        ),
        tags$div(
          style = "flex: 0 0 25%;",
          uiOutput("ui_figurecaption")
        ),
        tags$hr(
          style = "border: none; border-top: 1px solid #ccc; margin: 5px 0;"
        ),
        tags$div(
          style = "flex: 1;",
          uiOutput("ui_interpretation")
        )
      )
    } else {
      tags$div(
        tags$div(
          style = "flex: 0 0 25%",
          plotOutput('histogram_x')
        ),
        tags$div(
          style = "flex: 0 0 25%",
          plotOutput('histogram_y')
        )
      )
    }
  })

output$ui_dl_btn <- renderUI({
  req(calc_complete())
  downloadBttn(
              outputId = "download_report",
              style = "jelly",
              color = "primary"
            )
})

output$step3 = renderUI({
  req(graph_complete())
  tagList(
    hr(),
    h4("Step 2: Select your variables of interest:")
  )
})

output$xvar = renderUI({
  req(graph_complete())  
  selectInput('xvar', 'Experimental (X) variable:', c("", names(df())),
              selected = ""
              )

  # xvar = "scenario_main_base"
  # xvar = "humidity"
})

output$yvar = renderUI({
  req(input$xvar)  
  x_desc <- get_X_descendents(input$xvar, AIRHome)
  choices <- setdiff(x_desc, input$xvar)
  selectInput('yvar', 'Outcome (Y) variable:', c("",choices), 
              selected = ""
              )
  # yvar = "images_acquired"
})

output$ui_threshold_x = renderUI({
  req(input$xvar)
  slider_range <- c(min(df()[[input$xvar]]), max(df()[[input$xvar]]))
  slider_step <- (slider_range[2] - slider_range[1]) / 10
   # Use Flexbox for inline alignment
    tags$div(
      style = "display: flex; align-items: center; flex-wrap: wrap;",
      
      # Part 1: Variable Selection
      tags$span(""),
      tags$span("is considered treated when it is "),
      
      # Part 2: Operator Selection
      tags$div(
        style = "margin-right: 5px;",
        selectInput("tv_dir", NULL, 
                    choices = c(">", ">=", "<", "<=", "="), 
                    width = "60px")
        ),
      # Part 3: Threshold Input
      tags$div(
        style = "margin-right: 5px;",
        numericInput("tv_threshold", NULL, 
                     value = mean(slider_range), 
                     step = slider_step,
                     width = "80px")
        )
    )
  
    # tv_dir = "<="
    # tv_threshold = 0
})

output$ui_threshold_y = renderUI({
  req(input$yvar)
  slider_range <- c(min(df()[[input$yvar]]), max(df()[[input$yvar]]))
  slider_step <- (slider_range[2] - slider_range[1]) / 10
   # Use Flexbox for inline alignment
    tags$div(
      style = "display: flex; align-items: center; flex-wrap: wrap;",
      
      # Part 1: Variable Selection
      tags$span(""),
      tags$span("is considered a success when it is "),
      
      # Part 2: Operator Selection
      tags$div(
        style = "margin-right: 5px;",
        selectInput("ov_dir", NULL, 
                    choices = c(">", ">=", "<", "<=", "="), 
                    width = "60px")
        ),
      # Part 3: Threshold Input
      tags$div(
        style = "margin-right: 5px;",
        numericInput("ov_threshold", NULL, 
                     value = mean(slider_range), 
                     step = slider_step,
                     width = "80px")
        )
    )
  
    # ov_dir = "<="
    # ov_threshold = 0
})

output$step2 = renderUI({
  req(input$file1)
  h4("Upload knowledge file:")
})

output$ui_file2 = renderUI({
  req(input$file1)
  fileInput("file2", "", accept = ".csv")
})
output$histogram_x <- renderPlot({
  req(input$xvar, input$tv_dir, input$tv_threshold)
    
  p <- get_histogram_x(df(), input$xvar, input$tv_dir, input$tv_threshold)
  p
  }, bg = "transparent")

output$histogram_y <- renderPlot({
    req(input$yvar, input$ov_dir, input$ov_threshold)
  p <- get_histogram_y(df(), input$yvar, input$ov_dir, input$ov_threshold)
  p
  }, bg = "transparent")

output$ui_model_exist = renderUI({
  req(graph_update())
  radioGroupButtons(
    inputId = "model_exist",
    label = "Do you have an existing model?",
    choiceNames = c("Yes: I can upload it", "Yes: I can provide an ATE", "No: Do it all for me"),
    choiceValues = c("Yes", "ATE", "No"), 
    selected = "ATE",
    individual = TRUE,
    checkIcon = list(
      yes = tags$i(class = "fa fa-circle", 
                   style = "color: steelblue"),
      no = tags$i(class = "fa fa-circle-o", 
                  style = "color: steelblue"))
  )
})

output$ui_ate_upload = renderUI({
  req(graph_update())
  req(input$model_exist)
  if (input$model_exist == "ATE") {
    numericInput("ate_in", label = "ATE: ", value = 0, min = -1, max = 1, step = 0.1)
    # model_exist <- "ATE"
    # ate_in <- 0
  } else (return(NULL))
  
})

output$ui_model_upload = renderUI({
  req(input$model_exist)
  if (input$model_exist == "Yes") {
    fileInput("model_in", "", 
              accept = c(".rds", ".rda", ".model"))
  } else (return(NULL))
})

output$step4 = renderUI({
  req(graph_update())
  tagList(
    hr(),
    h4("Step 3: Tell us about your model:")
  )
})


output$ui_graphViz = renderUI({
  req(calc_complete())
  h4("Causal graph")
  grVizOutput("graphViz")
})

output$ui_ci_plot = renderUI({
  req(calc_complete())  
  h4("Comparison of ATE for AIR and ML")
  plotOutput("ci_plot")
})

output$ui_ribbon_plot <- renderUI({
  req(calc_complete())
  tagList(
    h4("Comparison of ATE for AIR and ML"),
    imageOutput("ribbon_plot")
  )
})

output$ui_ribbon_plot <- renderUI({
  req(calc_complete())
  tags$div(
    style = "max-width: 600px;
            margin: 0 auto;",
    h4("Comparison of ATE for AIR and ML"),
    # make the <img> responsive
    imageOutput("ribbon_plot",
                width  = "100%",
                height = "auto")
  )
})

output$ribbon_plot <- renderImage(
  {
    req(calc_complete())
    tmp <- tempfile(fileext = ".png")
    plt <- get_ribbon_plot(AIRHome)

    # bail out early if we didn't get an actual plot
    if (!inherits(plt, "gg")) {
      return(list(           # let Shiny show the <div> instead of an image
        src  = NULL,
        alt  = "No ribbon plot to display"
      ))
    }

    ggsave(tmp, plot = plt,
           width = 6.5, height = 4, units = "in", bg = "transparent")
           list(src         = tmp,
                contentType = "image/png",
                alt         = "ATE ribbon plot",
                width       = "100%",   # let CSS handle final size
                height      = NULL)     # keep aspect ratio
  },
  deleteFile = TRUE
)

output$ui_figurecaption = renderUI({
  req(calc_complete())
  uiOutput("figurecaption")
})

output$ui_goButton = renderUI({
  req(graph_update())
  actionBttn(
    inputId = "goButton",
    label = "Calculate Results",
    style = "jelly", 
    color = "primary"
  )
})

output$ui_buildButton = renderUI({
  req(file_check())
  actionBttn(
    inputId = "buildButton",
    label = "Build Graph",
    style = "jelly", 
    color = "primary"
    )
})

output$ui_updateButton = renderUI({
  req(input$yvar)
  actionBttn(
    inputId = "updateButton",
    label = "Update Graph",
    style = "jelly",
    color = "primary"
    )
})

output$my_plot <- renderImage({
  # Use magick to draw and save to temp file
  temp_file <- tempfile(fileext = ".png")
  img <- magick::image_graph(width = 800, height = 600, res = 100)
  print(my_ggplot())
  dev.off()
  magick::image_write(img, path = temp_file, format = "png")

  list(
    src = temp_file,
    contentType = "image/png",
    alt = "Generated plot"
  )
}, deleteFile = FALSE)

output$figurecaption = renderUI({
  req(calc_complete())
  caption <- get_figure_caption(AIRHome, df_vars)
  tags$div(style = "font-size:12px;", caption)
  })

output$ui_interpretation = renderUI({
  req(calc_complete())  

  result_text <- get_ui_interpretation(AIRHome, df_vars, Zvars)

  tagList(
    tags$h3("Interpreting your results:"),
    tags$div(style = "font-size:24px;", result_text)
  )
})

output$download_report <- downloadHandler(
  filename = function() {
    paste0('AIRTool-Report_', Sys.Date(), '.pdf')
  },
  content = function(file) {
    tmpdir <- tempfile("airtool_report_")
    dir.create(tmpdir, recursive = TRUE)

    # Pre-evaluate reactive values
    loc_xvar <- input$xvar
    loc_yvar <- input$yvar
    loc_df <- df()
    loc_tv_dir <- input$tv_dir
    loc_tv_threshold <- input$tv_threshold
    loc_ov_dir <- input$ov_dir
    loc_ov_threshold <- input$ov_threshold
    loc_df_vars <- df_vars
    loc_Zvars <- Zvars
    loc_graph_update <- graph_update()
    loc_figure_cap <- get_figure_caption(AIRHome, df_vars)
    loc_result_text <- get_ui_interpretation(AIRHome, df_vars, Zvars)

    # Draw ggplots to files using magick
    save_plot <- function(plot_func, path) {
      img <- magick::image_graph(width = 800, height = 600, res = 100)
      print(plot_func)
      dev.off()
      magick::image_write(img, path = path, format = "png")
    }

    save_plot(get_histogram_x(loc_df, loc_xvar, loc_tv_dir, loc_tv_threshold), file.path(tmpdir, "xhist.png"))
    save_plot(get_histogram_y(loc_df, loc_yvar, loc_ov_dir, loc_ov_threshold), file.path(tmpdir, "yhist.png"))
    save_plot(get_ribbon_plot(AIRHome), file.path(tmpdir, "ribbon.png"))

    # Save Graphviz images (assume save_dot_to_png already uses DiagrammeRsvg + magick::image_write)
    save_dot_to_png(get_updated_graph(AIRHome, loc_graph_update, loc_xvar, loc_yvar, loc_Zvars), file.path(tmpdir, "updatedgraph.png"))
    save_dot_to_png(get_final_graph(AIRHome, loc_xvar, loc_yvar, loc_Zvars), file.path(tmpdir, "finalgraph.png"))

    # Copy the .qmd into the tmpdir so quarto doesn't pollute source folder
    file.copy("scripts/AIRReport.qmd", file.path(tmpdir, "AIRReport.qmd"))

    # Render to tmpdir
    quarto::quarto_render(
      input = file.path(tmpdir, "AIRReport.qmd"),
      output_format = "pdf",
      output_file = "AIRReport.pdf",
      output_dir = tmpdir,
      execute_params = list(
        AIRHome = AIRHome,
        xvar = loc_xvar,
        yvar = loc_yvar,
        figure_cap = loc_figure_cap,
        result_text = loc_result_text
      )
    )

    final_path <- file.path(tmpdir, "AIRReport.pdf")
    if (!file.exists(final_path)) {
      stop("Failed to generate report")
    }

    # Deliver it to Shiny
    file.copy(final_path, file, overwrite = TRUE)
  }
)
```

# Key

-   Graph Colors
    -   [Yellow]{style="color: #FFC107; font-weight: bold;"}: Variable of interest, selected by the user in step 2. Both X and Y variables are shaded yellow to quickly draw the attention of the user to their selected variables.
    -   [Dark Gray]{style="color: #9394A2; font-weight: bold;"}: Any nodes with this color have been identified as belonging to the first identified adjustment set, used to calculate causal effect estimates.
    -   [Light Gray]{style="color: #D4C7C7; font-weight: bold;"}: Any nodes with this color have been identified as belonging to the second identified adjustment set, used to calculate causal effect estimates.
    -   [Red]{style="color: #C00000; font-weight: bold;"}: Any nodes with this color have been flagged as introducing bias into the results of the input classifier. Nodes will change from (dark/light) gray to red if the classifier ATE falls outside the 95% confidence interval for a given adjustment set.
-   Histogram Colors
    -   [Blue]{style="color: #5D9AFF; font-weight: bold;"}: This is the 'treated' or 'success' portion of the data. Data falling within this range are categorized as 1. This represents the presence of a treatment, success, category of interest, etc...
    -   [Gray]{style="color: #EAE1D7; font-weight: bold;"}: This is the 'untreated' or 'fail' portion of the data. Data falling within this range are categorized as 0. This represents the absence of a treatment, success, category of interest, etc...
-   Ribbon Plot Colors
    -   [Red]{style="color: #C00000; font-weight: bold;"}: The range of effect sizes that are outside the 95% confidence intervals of both adjustment sets. Values falling in this range are considered non-significant.
    -   [Yellow]{style="color: #FCB514; font-weight: bold;"}: The range of effect sizes that are outside of one 95% confidence interval for one effect size, but inside for the other. Values falling in this range are considered suspect, and should be closely monitored for signs of bias.
    -   [Green]{style="color: #378855; font-weight: bold;"}: The range of effect sizes that are inside both adjustment set's 95% confidence intervals. Values falling in this range are consistent with those calculated by AIR's causal estimation and are considered bias-free.
