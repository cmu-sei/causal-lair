---
format: html
resources:
  - readme_md_files
---

<h1>Introduction </h1>

-   **Brief Overview:** Modern analytic methods, including Artificial Intelligence (AI) and Machine Learning (ML) classifiers, depend on correlations; however, such approaches often fail to account for confounding in the data, which prevents accurate modeling of cause and effect. This often leads to prediction bias. The AI Robustness (AIR) tool allows users to gauge AI/ML classifier performance with unprecedented confidence.

-   **Target Audience:** Projects that have an established AI classifier workflow, complete with data dictionaries and subject-matter experts. These release notes are for potential partners that would like to install the AIR tool in their own environment.

<h1>New Features </h1>

-   **Detailed Descriptions:** Initial release.
-   **Benefits:** NA for this release.

<h1>Getting Started</h1>

**Step 1: Building your Causal Graph**

<!-- <img src="image1.png" style="width:6.5in; height:2.8in;" alt="Description of image1" /> -->
The tool will first prompt the user for their data file. This file should conform to the characteristics outlined in the "Model and Data Requirements" section above. It is most helpful if it is either the same data that was used to build the AI classifier or if it is data that could be fed to the AI classifier to make predictions.

After a data file is uploaded, the user will then select their knowledge file for upload. Knowledge files define rough hierarchies of three or more levels of causation as determined logically or by subject matter experts. Levels are defined as follows:

-   **Tier 0 — Exogenous variables:** These variables are not influenced by any other variables. Often used as starting points for causal graphs.
-   **Tier 1 — Endogenous variables:** Variables in this tier are potentially influenced by those in Tier 0 and possibly other Tier 1 variables.
-   **Tier 2 and up — Higher-tier variables:** These variables may be influenced by preceding tiers or even within the same tier. Although there can be any number of tiers, three tiers are strictly necessary.

Currently, all knowledge assertions must be done ahead of time by the user as in-place editing is not yet supported. The file format should be similar to the data file (i.e., CSV with a header), but will contain only two columns: `level` and `variable` (where `level` contains a numeric tier and `variable` contains the variable name exactly as in the data file). Each variable name should appear exactly once.

Once both files are uploaded and accepted, a new button "Build Graph" will appear. Clicking it will run causal discovery algorithms to build your causal graph and display it in the main panel. If you are unsatisfied with the graph and feel that updating your data or knowledge file might help, you can select new files and re-build your graph until satisfied.

**Step 2: Identifying potential sources of bias**

<!-- ```{r} -->
<!-- shiny::addResourcePath(prefix = "readme_md_files", directoryPath = "readme_md_files") -->
<!-- ``` -->
<!-- <img src="readme_md_files/image2.png" style="width:6.5in; height:2.8in;" alt="Description of image2"/> -->
<!-- ![](readme_md_files/image2.png) -->
The tool will now prompt users for additional information about the problem the classifier is attempting to solve. Most important is identifying both the experimental/treatment (x) and outcome (y) variables. Each variable definition will be pulled from the data file.

> **Note:** In the current version of the tool, both x and y variables must be treated as binary. Users will define what constitutes "treated" vs. "untreated" and "success" vs. "fail" for the x and y variables, respectively. Data distributions are displayed on the right of the setup pane to help visualize decision criteria.

Once the user has completed their definitions for the experimental/treatment (x) and outcome (y) variables, they may click the "Update Graph" button to proceed. Activating this button will run the causal identification algorithms in AIR, which will update the causal graph by highlighting: - Both x and y variables (in yellow) - Two separate adjustment sets: - Potential confounders that are parents of x and y (displayed in medium gray) - Potential confounders that are parents of x and intermediate variables and/or y (displayed in light gray)

As with Step 1, the user may continue editing until satisfied, but must always click "Update Graph" for changes to take effect.

**Step 3: Estimating the causal effect to compare with your AI Classifier**

<!-- <img src="./readme_md_files/image3.png" style="width:6.5in; height:2.8in;" alt="Description of image3" /> -->
The tool will now prompt the user for information about their classifier to be tested. This section is context-dependent, so the input boxes will change according to the user's selection. Currently, three options are available:

-   **Uploading a model:** This prompts the user to upload a copy of the model used to estimate the average treatment effect (ATE) predicted by the classifier. (Currently, only models in the `.rda` format are accepted; additional formats can be added upon request.)
-   **Providing an ATE:** If the user can calculate their own ATE, they may input that value directly. (See "Generating your own ATE" below for more details.)
-   **No information (do it all for me):** For users who don't have a specific model, the tool will generate several commonly used machine learning models and compare them against the causally-derived model of AIR. No additional input is required.

After making a selection, click the "Calculate Results" button to finish the causal estimation portion. Once initiated, the process cannot be undone, so ensure you are ready. In our trials with a fairly simple model, this process usually takes 2-5 minutes. Once complete, the progress bar will disappear and the view will navigate to the "Results" tab using the blue ribbon at the top of the tool screen.

<h2>Results</h2>

<!-- <img src="./readme_md_files/image4.png" style="width:6.5in; height:2.8in;" alt="Description of image4" /> -->
This page requires no user input but displays the full health report. It contains:

-   **Left:** The causal graph with both x and y variables highlighted in yellow. Additional nodes contributing significant bias (if found) will be highlighted in red, with further details provided in the "Interpreting your results" section.
-   **Top Right:** A 'ribbon plot' that displays a summary of the ATE and its associated 95% confidence interval for both adjustment sets (medium and light gray). Values within both intervals are shaded green; those within only one interval are yellow; and values outside both are red. The classifier's ATE is indicated by an arrow on the line. These causal intervals provide independent checks on classifier behavior. If one interval is violated, it may be a statistical anomaly; if both are violated, caution is advised regarding that use case. The adjustment sets output in Step 2 recommend which variables/features to focus on for subsequent classifier retraining.
-   **Bottom Right:** A custom text-based interpretation summarizing results from all steps. These interpretations are generated automatically and are unique to each session.

<h3>Generating your own ATE</h3>

If you provide your own ATE, the AIR Tool accepts values calculated using potential outcome prediction. In practice, you cannot observe both ($Y\_{1}$) (observed treated outcome) and ($Y\_{0}$) (observed untreated outcome) for the same individual. Instead, the model simulates these outcomes. For each individual:

-   ($\widehat{Y}\_{1}$): The predicted outcome when treatment ( T = 1 ) is manually set for all individuals.
-   ($\widehat{Y}\_{0}$): The predicted outcome when treatment ( T = 0 ) is manually set for all individuals.

The ATE is then computed as the average difference between these potential outcomes:
$$
ATE = \frac{1}{N}\sum*{i = 1}^{N}* \left( \widehat{Y}{1i} - \widehat{Y}\_{0i} \right)
$$
<h1>Known Issues/Limitations</h1>

**Existing Problems:** Issues that remain in the current release will be identified as testing continues.

**Limitations:**

-   The tool currently only handles binary (on/off or true/false) treatment and outcome variables. This is due to differences in analysis requirements. A built-in tool to transform continuous variables into binary format is provided as part of Step 2.
-   Only `.rda` files are accepted for model uploads. If you have another model format you’d like to use, please let us know.
-   The tool does not fix your model but provides a health report that identifies potential sources of bias. It is up to the user to apply appropriate remedies based on this information.

<h1>Contact and Support Information</h1>

-   **Support Channels:** How users can reach out for help (e.g., email, support portal).
-   **Feedback Mechanism:** [tailor-help\@sei.cmu.edu](mailto:tailor-help@sei.cmu.edu){.email}

<h1>Roadmap or Future Updates</h1>

-   **Upcoming Features:** Coming soon.
-   **Planned Enhancements:** Coming soon.

<h1>Security Information</h1>

In the current AIR tool, data is not saved or used beyond the purposes specified above. Once the tool has finished running, its state is not saved for future use. Users are responsible for the handling of their source data.

<h1>Licensing and Legal Information</h1>

-   **Licensing Terms:** Clarify usage rights and any licensing requirements.
-   **Legal Disclaimers:** Include any necessary legal notices.
